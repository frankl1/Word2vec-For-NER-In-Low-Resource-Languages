{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 88,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2799,
     "status": "ok",
     "timestamp": 1516045322673,
     "user": {
      "displayName": "Michael Franklin Mbouopda",
      "photoUrl": "//lh3.googleusercontent.com/-2_Vuj9ESsJ0/AAAAAAAAAAI/AAAAAAAABSc/r-SRcN0jWPw/s50-c-k-no/photo.jpg",
      "userId": "117384351130599115261"
     },
     "user_tz": -60
    },
    "id": "NamUCuj1bBjd",
    "outputId": "7874258f-7c3e-4646-f238-36437759767b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import keras\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "from keras.utils import np_utils, plot_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, precision_recall_fscore_support\n",
    "import h5py as h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "t9h7fyDCbkMG"
   },
   "outputs": [],
   "source": [
    "BINARY = False\n",
    "timestep = 1\n",
    "epochs = 10\n",
    "en_corpus_file = \"corpus-en.txt\"\n",
    "ewo_corpus_file = \"corpus-ewo.txt\"\n",
    "best_model_file = \"best-model-conll.hdfs\"\n",
    "max_nb_of_phrases =  -1\n",
    "duplication = 1\n",
    "max_depth = 0\n",
    "is_only_vocab = True\n",
    "time_history_default = None\n",
    "time_history_autoencoder = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "t9h7fyDCbkMG"
   },
   "outputs": [],
   "source": [
    "with_encoding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "q1OlcYPo1Att"
   },
   "outputs": [],
   "source": [
    "def getTag(aString):\n",
    "    tag = \"O\"\n",
    "    if BINARY:\n",
    "        if aString != \"O\":\n",
    "            return \"NE\"\n",
    "    else:\n",
    "        tag = aString\n",
    "    return tag\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "q1OlcYPo1Att"
   },
   "outputs": [],
   "source": [
    "def load_corpus(file, max_nb_of_phrases):\n",
    "    nb_of_phrases = 0\n",
    "    dataset = {\"word\": [], \"ne-tag\": []}\n",
    "    with open(file) as f:\n",
    "        prev_line = None\n",
    "        for cpt, line in enumerate(f):\n",
    "            if cpt == 0:\n",
    "                continue\n",
    "            if nb_of_phrases == max_nb_of_phrases:\n",
    "                break;\n",
    "\n",
    "            l = line.strip()\n",
    "            if len(l) == 0 and len(prev_line) != 0:\n",
    "                nb_of_phrases += 1\n",
    "                dataset[\"word\"].append(line)\n",
    "                dataset[\"ne-tag\"].append(None)\n",
    "            else:\n",
    "                l = l.split(\"\\t\")\n",
    "                dataset[\"word\"].append(l[0])\n",
    "                dataset[\"ne-tag\"].append(ne_type(l[1]))\n",
    "            prev_line = line.strip()\n",
    "        \n",
    "    return pd.DataFrame(dataset), nb_of_phrases+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "q1OlcYPo1Att"
   },
   "outputs": [],
   "source": [
    "def corpus_fingerprint(aDataframe, nb_of_biphrases):\n",
    "    # create distributionnal signature\n",
    "    fingerprints = {}\n",
    "    current_bi_phrase_index = 0\n",
    "    for index, row in aDataframe.iterrows():\n",
    "        if current_bi_phrase_index > nb_of_biphrases:\n",
    "            break\n",
    "            \n",
    "        word = row['word']\n",
    "        \n",
    "        if word != \"\\n\":\n",
    "            if word not in fingerprints:\n",
    "                fingerprints[word] = np.zeros(nb_of_biphrases, dtype=np.int8)\n",
    "            fingerprints[word][current_bi_phrase_index] = 1\n",
    "        else:\n",
    "            current_bi_phrase_index += 1\n",
    "    return pd.DataFrame(fingerprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "q1OlcYPo1Att"
   },
   "outputs": [],
   "source": [
    "def corpus2trainingdata(aDataframe, fingerprintsDataFrame):\n",
    "    X = np.zeros((aDataframe.shape[0], fingerprintsDataFrame.shape[0]), dtype=np.int8)\n",
    "    y = np.zeros(aDataframe.shape[0], dtype=np.int8)\n",
    "    i = 0\n",
    "    for row in aDataframe.iterrows():\n",
    "        X[i] = fingerprintsDataFrame[row[1]['word']].values\n",
    "        y[i] = tag2int[getTag(row[1]['ne-tag'])]\n",
    "        i += 1\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "q1OlcYPo1Att"
   },
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size = 0.33):\n",
    "    total = X.shape[0]\n",
    "    train_length = round(total * (1 - test_size)) \n",
    "    return X[:train_length], X[train_length:], y[:train_length], y[train_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "q1OlcYPo1Att"
   },
   "outputs": [],
   "source": [
    "  \n",
    "def ne_type(aType):\n",
    "    aType = aType.lower()\n",
    "    if 'per' in aType:\n",
    "        t =  'NE' if BINARY else 'PER' \n",
    "    elif 'loc' in aType:\n",
    "        t =  'NE' if BINARY else 'LOC'\n",
    "    elif 'org' in aType:\n",
    "        t =  'NE' if BINARY else 'ORG'\n",
    "    elif 'hour' in aType:\n",
    "        t =  'NE' if BINARY else 'MISC'\n",
    "    elif aType != 'o' and len(aType) > 0 :\n",
    "        t =  'NE' if BINARY else 'MISC'\n",
    "    else:\n",
    "        t = 'O'\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "q1OlcYPo1Att"
   },
   "outputs": [],
   "source": [
    "def compute_performance(y_true, y_pred, words=None, BINARY=False):\n",
    "    if BINARY:\n",
    "        p = precision_score(y_true, y_pred, pos_label=tag2int['NE'])\n",
    "        r = recall_score(y_true, y_pred, pos_label=tag2int['NE'])\n",
    "        f1 = f1_score(y_true, y_pred, pos_label=tag2int['NE'])\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "    else:\n",
    "        p = precision_score(y_pred, y_true, average='macro')\n",
    "        r = recall_score(y_pred, y_true, average='macro')\n",
    "        f1 = f1_score(y_pred, y_true, average='macro')\n",
    "        acc = accuracy_score(y_pred, y_true)\n",
    "    if words is None:\n",
    "        model_output_vs = pd.DataFrame({'y_true': [int2tag[i] for i in y_true], 'y_pred': [int2tag[i] for i in y_pred]})\n",
    "    else:\n",
    "        model_output_vs = pd.DataFrame({'word': words, 'y_true': [int2tag[i] for i in y_true], 'y_pred': [int2tag[i] for i in y_pred]})\n",
    "\n",
    "    return p, r, f1, acc, model_output_vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "q1OlcYPo1Att"
   },
   "outputs": [],
   "source": [
    "def P_R_F1(y_pred, y_true, neg_class):\n",
    "    same = y_pred[y_true==y_pred]\n",
    "    tp = same[same != neg_class].size\n",
    "    nb_of_pos_exple = y_true[y_true != neg_class].size\n",
    "    nb_of_pos_pred = y_pred[y_pred != neg_class].size\n",
    "    p = r = f1 = 0\n",
    "    try:\n",
    "        p = np.round(tp*100/nb_of_pos_pred, 2)\n",
    "    except ZeroDivisionError:\n",
    "        print(\"number of correct positive predictions is 0\")\n",
    "        \n",
    "    try:\n",
    "        r = np.round(tp*100/nb_of_pos_exple, 2)\n",
    "    except ZeroDivisionError:\n",
    "        print(\"number of position exple is 0\")\n",
    "        \n",
    "    try:\n",
    "        f1 = np.round(2*r*p/(r+p), 2)\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Recall and precision are 0\")\n",
    "\n",
    "    return p, r, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_mergeable(aListOfConsecutiveTokens, corpus, fingerprints):\n",
    "    n = len(aListOfConsecutiveTokens)\n",
    "    if n <= 1:\n",
    "        return False\n",
    "    if n == 2:\n",
    "        w1, w2 = aListOfConsecutiveTokens[0], aListOfConsecutiveTokens[1]\n",
    "        rep1, rep2 = fingerprints[aListOfConsecutiveTokens[0]], fingerprints[aListOfConsecutiveTokens[1]]\n",
    "        tag1, tag2 = corpus[corpus.word==w1].iloc[0]['ne-tag'], corpus[corpus.word==w2].iloc[0]['ne-tag']\n",
    "        if (tag1 == tag2) and (tag1 == \"O\"): # O + O => False\n",
    "            return False\n",
    "        if (tag1 != tag2) and (tag1 != \"O\") and (tag2 != \"O\"): # X + Y => False\n",
    "            return False\n",
    "        return rep1.equals(rep2)\n",
    "    else:\n",
    "        half = int(n / 2)\n",
    "        return is_mergeable(aListOfConsecutiveTokens[0:half+1], corpus, fingerprints) and is_mergeable(aListOfConsecutiveTokens[half:n], corpus, fingerprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "q1OlcYPo1Att"
   },
   "outputs": [],
   "source": [
    "def merge(depth, corpus, fingerprint):\n",
    "    wordDf = corpus[corpus.word != \"\\n\"].word\n",
    "    nbOfWord = wordDf.shape[0]\n",
    "    text = list(wordDf)\n",
    "    X2, target2, tokens = [], [], []\n",
    "    level, newToken = 1, True\n",
    "    while level <= depth and newToken:\n",
    "        i, newToken = 0, False\n",
    "        limit = nbOfWord - level\n",
    "        while i < limit:\n",
    "            if is_mergeable(text[i:i+level+1], corpus, fingerprint):\n",
    "                tokens.append(\" \".join(text[i:i+level+1]))\n",
    "                newToken = True\n",
    "            i += 1\n",
    "        print(\"level \", level, \":\", set(tokens))\n",
    "        level += 1\n",
    "    \n",
    "    X2, target2 = np.array(X2), np.array(target2)\n",
    "    \n",
    "    return X2, target2, set(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X, y):\n",
    "    indices = [i for i in  range(X.shape[0])]\n",
    "    np.random.shuffle(indices)\n",
    "    return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "\n",
    "def create_and_train_autoencoder(X_train, code_dim, epochs=10):\n",
    "    input_data = Input(shape=(X_train.shape[1],))\n",
    "    encoded = Dense(code_dim, activation=\"sigmoid\")(input_data)\n",
    "    decoded = Dense(X_train.shape[1], activation=\"sigmoid\")(encoded)\n",
    "    \n",
    "    encoder = Model(input_data, encoded)\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "    autoencoder.compile(optimizer=\"sgd\", loss=\"binary_crossentropy\")\n",
    "    autoencoder.fit(X_train, X_train, shuffle=True, epochs=epochs, validation_data=(X_train, X_train))\n",
    "    \n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TNN2TBckE8m_"
   },
   "outputs": [],
   "source": [
    "en_corpus, en_nb_of_phrases = load_corpus(en_corpus_file, max_nb_of_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'MISC': 1, 'PER': 2, 'LOC': 3, 'ORG': 4}\n"
     ]
    }
   ],
   "source": [
    "tagSet = en_corpus[\"ne-tag\"].dropna().unique()\n",
    "if BINARY:\n",
    "    tagSet = ['NE', 'O']\n",
    "tag2int = {j: i for i, j in enumerate(tagSet)}\n",
    "int2tag = {i: j for i, j in enumerate(tagSet)}\n",
    "print(tag2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 794,
     "status": "ok",
     "timestamp": 1515664141558,
     "user": {
      "displayName": "Michael Franklin Mbouopda",
      "photoUrl": "//lh3.googleusercontent.com/-2_Vuj9ESsJ0/AAAAAAAAAAI/AAAAAAAABSc/r-SRcN0jWPw/s50-c-k-no/photo.jpg",
      "userId": "117384351130599115261"
     },
     "user_tz": -60
    },
    "id": "m85WcghdzCph",
    "outputId": "9fa6817e-15c4-4205-f8f4-82c30c2cb610"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_nb_of_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 992,
     "status": "ok",
     "timestamp": 1515664144298,
     "user": {
      "displayName": "Michael Franklin Mbouopda",
      "photoUrl": "//lh3.googleusercontent.com/-2_Vuj9ESsJ0/AAAAAAAAAAI/AAAAAAAABSc/r-SRcN0jWPw/s50-c-k-no/photo.jpg",
      "userId": "117384351130599115261"
     },
     "user_tz": -60
    },
    "id": "Gw9r-Q9jlmvg",
    "outputId": "8ee33794-5639-4c97-ea43-06a66d89e207"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ne-tag</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4753</td>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5</td>\n",
       "      <td>913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>O</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4362</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ne-tag  word\n",
       "count    4753  4962\n",
       "unique      5   913\n",
       "top         O     ,\n",
       "freq     4362   343"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_corpus.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1056,
     "status": "ok",
     "timestamp": 1515664147270,
     "user": {
      "displayName": "Michael Franklin Mbouopda",
      "photoUrl": "//lh3.googleusercontent.com/-2_Vuj9ESsJ0/AAAAAAAAAAI/AAAAAAAABSc/r-SRcN0jWPw/s50-c-k-no/photo.jpg",
      "userId": "117384351130599115261"
     },
     "user_tz": -60
    },
    "id": "EZ_5FqH3yxhU",
    "outputId": "a129592b-9fa2-4937-a35a-a73245aef4a8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ne-tag</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O</td>\n",
       "      <td>Promise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MISC</td>\n",
       "      <td>Holy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MISC</td>\n",
       "      <td>Spirit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>None</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>O</td>\n",
       "      <td>In</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>O</td>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ne-tag     word\n",
       "0      O      The\n",
       "1      O  Promise\n",
       "2      O       of\n",
       "3      O      the\n",
       "4   MISC     Holy\n",
       "5   MISC   Spirit\n",
       "6   None       \\n\n",
       "7      O       In\n",
       "8      O      the\n",
       "9      O    first"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_corpus.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O % = 87.91 %\n",
      "MISC % = 2.12 %\n",
      "PER % = 4.94 %\n",
      "LOC % = 0.81 %\n",
      "ORG % = 0.02 %\n"
     ]
    }
   ],
   "source": [
    "for tag in tagSet:\n",
    "    print(\"{0} % = {1} %\".format(tag, np.round(en_corpus[en_corpus['ne-tag']==tag].shape[0] * 100 / en_corpus[en_corpus['ne-tag']!='\\n'].shape[0], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O % = 89.27 %\n",
      "MISC % = 1.86 %\n",
      "PER % = 8.87 %\n",
      "LOC % = 1.97 %\n",
      "ORG % = 0.11 %\n"
     ]
    }
   ],
   "source": [
    "for tag in tagSet:\n",
    "    print(\"{0} % = {1} %\".format(tag, np.round(en_corpus[en_corpus['ne-tag']==tag].word.unique().shape[0] * 100 / en_corpus[en_corpus['ne-tag']!='\\n'].word.unique().shape[0], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_corpus[en_corpus.word == \"\\n\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 190,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40378,
     "status": "ok",
     "timestamp": 1515671247748,
     "user": {
      "displayName": "Michael Franklin Mbouopda",
      "photoUrl": "//lh3.googleusercontent.com/-2_Vuj9ESsJ0/AAAAAAAAAAI/AAAAAAAABSc/r-SRcN0jWPw/s50-c-k-no/photo.jpg",
      "userId": "117384351130599115261"
     },
     "user_tz": -60
    },
    "id": "Mx1Yl-Npvh12",
    "outputId": "29d26922-46c0-47b6-9d1b-26ac26b118d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of bi-phrases 210\n"
     ]
    }
   ],
   "source": [
    "print(\"Nb of bi-phrases\", en_nb_of_phrases)\n",
    "en_fingerprints = corpus_fingerprint(en_corpus, en_nb_of_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4753, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_corpus[en_corpus.word != \"\\n\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2, target2, tokens = merge(max_depth, en_corpus, en_fingerprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>'The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text\n",
       "count    912\n",
       "unique   912\n",
       "top     'The\n",
       "freq       1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if is_only_vocab:\n",
    "    text = list(en_corpus[en_corpus.word != \"\\n\"].word.unique())\n",
    "else:\n",
    "    text = list(en_corpus[en_corpus.word != \"\\n\"].word)\n",
    "en_vocab = pd.DataFrame({'text': text + list(tokens)})\n",
    "en_vocab.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 190,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40378,
     "status": "ok",
     "timestamp": 1515671247748,
     "user": {
      "displayName": "Michael Franklin Mbouopda",
      "photoUrl": "//lh3.googleusercontent.com/-2_Vuj9ESsJ0/AAAAAAAAAAI/AAAAAAAABSc/r-SRcN0jWPw/s50-c-k-no/photo.jpg",
      "userId": "117384351130599115261"
     },
     "user_tz": -60
    },
    "id": "Mx1Yl-Npvh12",
    "outputId": "29d26922-46c0-47b6-9d1b-26ac26b118d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(912, 210) (210, 912) (912,)\n"
     ]
    }
   ],
   "source": [
    "if is_only_vocab:\n",
    "    X = np.zeros((en_vocab.shape[0] * duplication, en_nb_of_phrases))\n",
    "    target = np.zeros((en_vocab.shape[0] * duplication))\n",
    "    p=0\n",
    "    for i, row in en_vocab.iterrows():\n",
    "        c = row.text\n",
    "        for j in range(duplication):\n",
    "            X[p] = en_fingerprints[c.split(\" \")[0]]\n",
    "            target[p] = tag2int[getTag(en_corpus[en_corpus.word == c.split(\" \")[-1:][0]]['ne-tag'].iloc[0])]\n",
    "            p+=1\n",
    "    X, target = shuffle(X, target)\n",
    "    print(X.shape, en_fingerprints.shape, target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>Eliud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>Eleazar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>Matthan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>husband</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>fourteen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>unwilling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>shame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>resolved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>divorce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>quietly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>considered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>dream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>She</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>save</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>fulfill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>Immanuel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>us)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>woke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>sleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>knew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text\n",
       "892       Eliud\n",
       "893     Eleazar\n",
       "894     Matthan\n",
       "895     husband\n",
       "896    fourteen\n",
       "897   unwilling\n",
       "898       shame\n",
       "899    resolved\n",
       "900     divorce\n",
       "901     quietly\n",
       "902  considered\n",
       "903       dream\n",
       "904         She\n",
       "905        save\n",
       "906     fulfill\n",
       "907    Immanuel\n",
       "908         us)\n",
       "909        woke\n",
       "910       sleep\n",
       "911        knew"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_only_vocab:\n",
    "    X, target = corpus2trainingdata(en_corpus[en_corpus.word != \"\\n\"], en_fingerprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(912, 210) (912,)\n",
      "(912, 210) (912,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, target.shape)\n",
    "if len(X.shape) == len(X2.shape):\n",
    "    X = np.concatenate((X, X2))\n",
    "    target = np.concatenate((target, target2))\n",
    "    X, target = shuffle(X, target)\n",
    "print(X.shape, target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 912 samples, validate on 912 samples\n",
      "Epoch 1/20\n",
      "912/912 [==============================] - 0s 205us/step - loss: 0.6866 - val_loss: 0.6821\n",
      "Epoch 2/20\n",
      "912/912 [==============================] - 0s 104us/step - loss: 0.6781 - val_loss: 0.6737\n",
      "Epoch 3/20\n",
      "912/912 [==============================] - 0s 137us/step - loss: 0.6698 - val_loss: 0.6655\n",
      "Epoch 4/20\n",
      "912/912 [==============================] - 0s 117us/step - loss: 0.6617 - val_loss: 0.6574\n",
      "Epoch 5/20\n",
      "912/912 [==============================] - 0s 100us/step - loss: 0.6537 - val_loss: 0.6495\n",
      "Epoch 6/20\n",
      "912/912 [==============================] - 0s 101us/step - loss: 0.6458 - val_loss: 0.6417\n",
      "Epoch 7/20\n",
      "912/912 [==============================] - 0s 102us/step - loss: 0.6380 - val_loss: 0.6340\n",
      "Epoch 8/20\n",
      "912/912 [==============================] - 0s 105us/step - loss: 0.6305 - val_loss: 0.6265\n",
      "Epoch 9/20\n",
      "912/912 [==============================] - 0s 98us/step - loss: 0.6230 - val_loss: 0.6191\n",
      "Epoch 10/20\n",
      "912/912 [==============================] - 0s 101us/step - loss: 0.6157 - val_loss: 0.6119\n",
      "Epoch 11/20\n",
      "912/912 [==============================] - 0s 99us/step - loss: 0.6085 - val_loss: 0.6048\n",
      "Epoch 12/20\n",
      "912/912 [==============================] - 0s 101us/step - loss: 0.6014 - val_loss: 0.5978\n",
      "Epoch 13/20\n",
      "912/912 [==============================] - 0s 97us/step - loss: 0.5945 - val_loss: 0.5909\n",
      "Epoch 14/20\n",
      "912/912 [==============================] - 0s 97us/step - loss: 0.5877 - val_loss: 0.5842\n",
      "Epoch 15/20\n",
      "912/912 [==============================] - 0s 98us/step - loss: 0.5810 - val_loss: 0.5775\n",
      "Epoch 16/20\n",
      "912/912 [==============================] - 0s 98us/step - loss: 0.5745 - val_loss: 0.5710\n",
      "Epoch 17/20\n",
      "912/912 [==============================] - 0s 99us/step - loss: 0.5680 - val_loss: 0.5647\n",
      "Epoch 18/20\n",
      "912/912 [==============================] - 0s 98us/step - loss: 0.5617 - val_loss: 0.5584\n",
      "Epoch 19/20\n",
      "912/912 [==============================] - 0s 101us/step - loss: 0.5554 - val_loss: 0.5522\n",
      "Epoch 20/20\n",
      "912/912 [==============================] - 0s 123us/step - loss: 0.5494 - val_loss: 0.5462\n"
     ]
    }
   ],
   "source": [
    "#encoding\n",
    "encoder = None\n",
    "if with_encoding:\n",
    "    encoder = create_and_train_autoencoder(X_train=X, code_dim=105, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5010538  0.5199639  0.4737534  0.52320796 0.47564256 0.4860164\n",
      " 0.5240697  0.4661537  0.514448   0.47302967 0.485101   0.47416255\n",
      " 0.5182444  0.5270213  0.4951151  0.5332668  0.4826507  0.49870887\n",
      " 0.48076454 0.49909675 0.48698178 0.5241947  0.512829   0.50001186\n",
      " 0.49207127 0.48856372 0.4942264  0.48658746 0.5177445  0.49236965\n",
      " 0.47789487 0.50547916 0.5054161  0.51976484 0.5197955  0.5245732\n",
      " 0.4702894  0.4847364  0.47987992 0.52928823 0.47252893 0.49416646\n",
      " 0.5258123  0.47891164 0.52332985 0.50714856 0.49364102 0.49662924\n",
      " 0.4970579  0.47251898 0.5279611  0.46865606 0.49206945 0.51863617\n",
      " 0.47213987 0.52018005 0.47855583 0.53179616 0.47714448 0.50584507\n",
      " 0.51492405 0.4991513  0.5107546  0.47312912 0.4979839  0.4807723\n",
      " 0.5229389  0.48166472 0.49972653 0.5100054  0.46705428 0.4916456\n",
      " 0.4862315  0.5317805  0.48293808 0.47212243 0.50023955 0.49912018\n",
      " 0.46805516 0.5124829  0.49893966 0.5027977  0.4673353  0.5294722\n",
      " 0.5340996  0.4720932  0.5139049  0.48675826 0.50410914 0.5067928\n",
      " 0.52087885 0.5051718  0.49221706 0.51660323 0.50693464 0.5107694\n",
      " 0.4938925  0.5198087  0.5091495  0.5079922  0.48623025 0.49578285\n",
      " 0.47975618 0.47468358 0.4986816 ]\n"
     ]
    }
   ],
   "source": [
    "X_encoded = X.copy()\n",
    "if with_encoding:\n",
    "    X_encoded = encoder.predict(X)\n",
    "    print(X_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 102,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1032,
     "status": "ok",
     "timestamp": 1515671250872,
     "user": {
      "displayName": "Michael Franklin Mbouopda",
      "photoUrl": "//lh3.googleusercontent.com/-2_Vuj9ESsJ0/AAAAAAAAAAI/AAAAAAAABSc/r-SRcN0jWPw/s50-c-k-no/photo.jpg",
      "userId": "117384351130599115261"
     },
     "user_tz": -60
    },
    "id": "HLn_an5ExZSC",
    "outputId": "382c3159-7917-40e3-e469-27c244d86663"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(912, 5)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = target.copy()\n",
    "y[0:100]\n",
    "if not BINARY:\n",
    "    y = np_utils.to_categorical(y, len(tagSet))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 119,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 899,
     "status": "ok",
     "timestamp": 1515671252070,
     "user": {
      "displayName": "Michael Franklin Mbouopda",
      "photoUrl": "//lh3.googleusercontent.com/-2_Vuj9ESsJ0/AAAAAAAAAAI/AAAAAAAABSc/r-SRcN0jWPw/s50-c-k-no/photo.jpg",
      "userId": "117384351130599115261"
     },
     "user_tz": -60
    },
    "id": "sF3M9I53xlY1",
    "outputId": "8d011439-2df0-4440-b334-7e21e44c2208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (611, 1, 105)\n",
      "y_train.shape = (611, 5)\n",
      "X_val.shape = (301, 1, 105)\n",
      "y_val.shape = (301, 5)\n",
      "O % in training data = 88.22 %\n",
      "O % in validation data = 88.7 %\n",
      "MISC % in training data = 0.65 %\n",
      "MISC % in validation data = 2.33 %\n",
      "PER % in training data = 9.0 %\n",
      "PER % in validation data = 7.31 %\n",
      "LOC % in training data = 2.13 %\n",
      "LOC % in validation data = 1.33 %\n",
      "ORG % in training data = 0.0 %\n",
      "ORG % in validation data = 0.33 %\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = model_selection.train_test_split(X_encoded, y, test_size=0.33)\n",
    "X_train = X_train.reshape(X_train.shape[0], timestep, X_train.shape[1])\n",
    "X_val = X_val.reshape(X_val.shape[0], timestep, X_val.shape[1])\n",
    "print(\"X_train.shape =\", X_train.shape)\n",
    "print(\"y_train.shape =\", y_train.shape)\n",
    "print(\"X_val.shape =\", X_val.shape)\n",
    "print(\"y_val.shape =\", y_val.shape)\n",
    "\n",
    "tTarget = np.array([np.argmax(yy) for yy in y_train])\n",
    "vTarget = np.array([np.argmax(yy) for yy in y_val])\n",
    "\n",
    "for tag in tagSet:\n",
    "    print(\"{0} % in training data = {1} %\".format(tag, np.round(tTarget[tTarget==tag2int[tag]].size * 100 / tTarget.shape[0], 2)))\n",
    "    print(\"{0} % in validation data = {1} %\".format(tag, np.round(vTarget[vTarget==tag2int[tag]].size * 100 / vTarget.shape[0], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 238,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1145,
     "status": "ok",
     "timestamp": 1515671862146,
     "user": {
      "displayName": "Michael Franklin Mbouopda",
      "photoUrl": "//lh3.googleusercontent.com/-2_Vuj9ESsJ0/AAAAAAAAAAI/AAAAAAAABSc/r-SRcN0jWPw/s50-c-k-no/photo.jpg",
      "userId": "117384351130599115261"
     },
     "user_tz": -60
    },
    "id": "cvpl6zMzxm6X",
    "outputId": "ef520c78-0fd3-494a-defd-fb3e2f01d371"
   },
   "outputs": [],
   "source": [
    "def create_model(input_dim, output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(640, input_shape=(None, input_dim), activation='sigmoid'))\n",
    "    model.add(Dense(160, activation='sigmoid'))\n",
    "    if BINARY:\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['binary_accuracy'])\n",
    "    else:\n",
    "        model.add(Dense(output_dim, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=\"rmsprop\", metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 343,
     "output_extras": [
      {
       "item_id": 99
      },
      {
       "item_id": 223
      },
      {
       "item_id": 284
      },
      {
       "item_id": 347
      },
      {
       "item_id": 394
      },
      {
       "item_id": 445
      },
      {
       "item_id": 490
      },
      {
       "item_id": 542
      },
      {
       "item_id": 590
      },
      {
       "item_id": 639
      },
      {
       "item_id": 684
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 676115,
     "status": "ok",
     "timestamp": 1515672664054,
     "user": {
      "displayName": "Michael Franklin Mbouopda",
      "photoUrl": "//lh3.googleusercontent.com/-2_Vuj9ESsJ0/AAAAAAAAAAI/AAAAAAAABSc/r-SRcN0jWPw/s50-c-k-no/photo.jpg",
      "userId": "117384351130599115261"
     },
     "user_tz": -60
    },
    "id": "uY2-NcCZxpXe",
    "outputId": "adcd5c23-d0c6-463a-bc11-db5d415eba1d"
   },
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, epochs=epochs):\n",
    "    time_history_cb = TimeHistory()\n",
    "    best_model_cp = ModelCheckpoint(best_model_file, save_best_only=True, verbose=2) # saved best model\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, callbacks=[best_model_cp, time_history_cb])\n",
    "    best_model = keras.models.load_model(best_model_file) #loading the best model\n",
    "    return best_model, time_history_cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X, y, binary=BINARY):\n",
    "    if BINARY:\n",
    "        y_pred = np.round(model.predict(X))\n",
    "        y_true = y\n",
    "    else:\n",
    "        predictions = model.predict(X)\n",
    "        y_pred = np.array([np.argmax(p) for p in predictions])\n",
    "        y_true = np.array([np.argmax(t) for t in y ])\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23179,
     "status": "ok",
     "timestamp": 1515672689915,
     "user": {
      "displayName": "Michael Franklin Mbouopda",
      "photoUrl": "//lh3.googleusercontent.com/-2_Vuj9ESsJ0/AAAAAAAAAAI/AAAAAAAABSc/r-SRcN0jWPw/s50-c-k-no/photo.jpg",
      "userId": "117384351130599115261"
     },
     "user_tz": -60
    },
    "id": "N33cmgKEyPOb",
    "outputId": "917cdaec-b68c-47d5-a8ea-bd6ef7387a67"
   },
   "outputs": [],
   "source": [
    "def model_performance(y_true, y_pred):\n",
    "    return P_R_F1(y_pred, y_true, tag2int['O']) #precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performace_by_tag(y_true, y_pred, tag):\n",
    "    p, r, f1 = 0, 0, 0\n",
    "    \n",
    "    eq = y_pred[y_pred==y_true]\n",
    "    correctly_pred = eq[eq==tag].size\n",
    "    try:\n",
    "        p = np.round(100 * correctly_pred / y_pred[y_pred==tag].size, 2)\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        r = np.round(100 * correctly_pred / y_true[y_true==tag].size, 2)\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        f1 = np.round(2 * r * p / (r + p), 2)\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "    \n",
    "    return p, r, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 758,
     "status": "ok",
     "timestamp": 1515664313240,
     "user": {
      "displayName": "Michael Franklin Mbouopda",
      "photoUrl": "//lh3.googleusercontent.com/-2_Vuj9ESsJ0/AAAAAAAAAAI/AAAAAAAABSc/r-SRcN0jWPw/s50-c-k-no/photo.jpg",
      "userId": "117384351130599115261"
     },
     "user_tz": -60
    },
    "id": "OObRLXTr268N",
    "outputId": "6361a898-fd04-4e15-8aa7-936a12b3221a"
   },
   "outputs": [],
   "source": [
    "ewo_corpus, ewo_nb_of_phrases = load_corpus(ewo_corpus_file, max_nb_of_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 758,
     "status": "ok",
     "timestamp": 1515664313240,
     "user": {
      "displayName": "Michael Franklin Mbouopda",
      "photoUrl": "//lh3.googleusercontent.com/-2_Vuj9ESsJ0/AAAAAAAAAAI/AAAAAAAABSc/r-SRcN0jWPw/s50-c-k-no/photo.jpg",
      "userId": "117384351130599115261"
     },
     "user_tz": -60
    },
    "id": "OObRLXTr268N",
    "outputId": "6361a898-fd04-4e15-8aa7-936a12b3221a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ewo_nb_of_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O % = 86.37 %\n",
      "MISC % = 2.18 %\n",
      "PER % = 5.76 %\n",
      "LOC % = 0.89 %\n",
      "ORG % = 0.05 %\n"
     ]
    }
   ],
   "source": [
    "for tag in tagSet:\n",
    "    print(\"{0} % = {1} %\".format(tag, np.round(ewo_corpus[ewo_corpus['ne-tag']==tag].shape[0] * 100 / ewo_corpus[ewo_corpus['ne-tag']!='\\n'].shape[0], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O % = 90.0 %\n",
      "MISC % = 1.17 %\n",
      "PER % = 8.25 %\n",
      "LOC % = 1.84 %\n",
      "ORG % = 0.19 %\n"
     ]
    }
   ],
   "source": [
    "for tag in tagSet:\n",
    "    print(\"{0} % = {1} %\".format(tag, np.round(ewo_corpus[ewo_corpus['ne-tag']==tag].word.unique().shape[0] * 100 / ewo_corpus[ewo_corpus['ne-tag']!='\\n'].word.unique().shape[0], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 758,
     "status": "ok",
     "timestamp": 1515664313240,
     "user": {
      "displayName": "Michael Franklin Mbouopda",
      "photoUrl": "//lh3.googleusercontent.com/-2_Vuj9ESsJ0/AAAAAAAAAAI/AAAAAAAABSc/r-SRcN0jWPw/s50-c-k-no/photo.jpg",
      "userId": "117384351130599115261"
     },
     "user_tz": -60
    },
    "id": "OObRLXTr268N",
    "outputId": "6361a898-fd04-4e15-8aa7-936a12b3221a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ne-tag</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4185</td>\n",
       "      <td>4394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5</td>\n",
       "      <td>1030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>O</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3795</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ne-tag  word\n",
       "count    4185  4394\n",
       "unique      5  1030\n",
       "top         O     ,\n",
       "freq     3795   413"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ewo_corpus.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 758,
     "status": "ok",
     "timestamp": 1515664313240,
     "user": {
      "displayName": "Michael Franklin Mbouopda",
      "photoUrl": "//lh3.googleusercontent.com/-2_Vuj9ESsJ0/AAAAAAAAAAI/AAAAAAAABSc/r-SRcN0jWPw/s50-c-k-no/photo.jpg",
      "userId": "117384351130599115261"
     },
     "user_tz": -60
    },
    "id": "OObRLXTr268N",
    "outputId": "6361a898-fd04-4e15-8aa7-936a12b3221a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ne-tag</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MISC</td>\n",
       "      <td>Mfufub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>Nsisim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O</td>\n",
       "      <td>ayi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ne-tag    word\n",
       "0   MISC  Mfufub\n",
       "1   MISC  Nsisim\n",
       "2      O     ayi\n",
       "3      O      s\n",
       "4   None      \\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ewo_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 214,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1473,
     "status": "error",
     "timestamp": 1515688720429,
     "user": {
      "displayName": "Michael Franklin Mbouopda",
      "photoUrl": "//lh3.googleusercontent.com/-2_Vuj9ESsJ0/AAAAAAAAAAI/AAAAAAAABSc/r-SRcN0jWPw/s50-c-k-no/photo.jpg",
      "userId": "117384351130599115261"
     },
     "user_tz": -60
    },
    "id": "aRDNM0C8yX5N",
    "outputId": "b5a14626-4a9b-484c-c597-f1b1f37673c9"
   },
   "outputs": [],
   "source": [
    "ewo_fingerprints = corpus_fingerprint(ewo_corpus, en_nb_of_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 214,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1473,
     "status": "error",
     "timestamp": 1515688720429,
     "user": {
      "displayName": "Michael Franklin Mbouopda",
      "photoUrl": "//lh3.googleusercontent.com/-2_Vuj9ESsJ0/AAAAAAAAAAI/AAAAAAAABSc/r-SRcN0jWPw/s50-c-k-no/photo.jpg",
      "userId": "117384351130599115261"
     },
     "user_tz": -60
    },
    "id": "aRDNM0C8yX5N",
    "outputId": "b5a14626-4a9b-484c-c597-f1b1f37673c9"
   },
   "outputs": [],
   "source": [
    "X2, target2, tokens = merge(max_depth, ewo_corpus, ewo_fingerprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 214,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1473,
     "status": "error",
     "timestamp": 1515688720429,
     "user": {
      "displayName": "Michael Franklin Mbouopda",
      "photoUrl": "//lh3.googleusercontent.com/-2_Vuj9ESsJ0/AAAAAAAAAAI/AAAAAAAABSc/r-SRcN0jWPw/s50-c-k-no/photo.jpg",
      "userId": "117384351130599115261"
     },
     "user_tz": -60
    },
    "id": "aRDNM0C8yX5N",
    "outputId": "b5a14626-4a9b-484c-c597-f1b1f37673c9"
   },
   "outputs": [],
   "source": [
    "if is_only_vocab:\n",
    "    text = list(ewo_corpus[ewo_corpus.word != \"\\n\"].word.unique())\n",
    "else:\n",
    "    text = list(ewo_corpus[ewo_corpus.word != \"\\n\"].word)\n",
    "ewo_vocab = pd.DataFrame({\"text\":text + list(tokens)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 214,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1473,
     "status": "error",
     "timestamp": 1515688720429,
     "user": {
      "displayName": "Michael Franklin Mbouopda",
      "photoUrl": "//lh3.googleusercontent.com/-2_Vuj9ESsJ0/AAAAAAAAAAI/AAAAAAAABSc/r-SRcN0jWPw/s50-c-k-no/photo.jpg",
      "userId": "117384351130599115261"
     },
     "user_tz": -60
    },
    "id": "aRDNM0C8yX5N",
    "outputId": "b5a14626-4a9b-484c-c597-f1b1f37673c9"
   },
   "outputs": [],
   "source": [
    "if is_only_vocab:\n",
    "    X_ewo = np.zeros((ewo_vocab.shape[0] * duplication, en_nb_of_phrases))\n",
    "    ewo_target = np.zeros((ewo_vocab.shape[0] * duplication))\n",
    "    p=0\n",
    "    for i, row in ewo_vocab.iterrows():\n",
    "        c = row.text\n",
    "        for j in range(duplication):\n",
    "            X_ewo[p] = ewo_fingerprints[c.split(\" \")[0]]\n",
    "            ewo_target[p] = tag2int[getTag(ewo_corpus[ewo_corpus.word == c.split(\" \")[-1:][0]]['ne-tag'].iloc[0])]\n",
    "            p+=1\n",
    "    X_ewo, ewo_target = shuffle(X_ewo, ewo_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>nlo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>ob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>mbara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>yabyali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>dzili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>yas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>oyolge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>kode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>dili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>atoban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>sik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>Ntud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>byole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>Emmanuel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>Avb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>angavb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>oy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>angabende</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>angan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>angayole</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text\n",
       "1009        nlo\n",
       "1010        ob\n",
       "1011      mbara\n",
       "1012    yabyali\n",
       "1013      dzili\n",
       "1014       yas\n",
       "1015    oyolge\n",
       "1016       kode\n",
       "1017       dili\n",
       "1018     atoban\n",
       "1019        sik\n",
       "1020       Ntud\n",
       "1021     byole\n",
       "1022   Emmanuel\n",
       "1023      Avb\n",
       "1024   angavb\n",
       "1025        oy\n",
       "1026  angabende\n",
       "1027    angan\n",
       "1028   angayole"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ewo_vocab[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_only_vocab:\n",
    "    X_ewo, ewo_target = corpus2trainingdata(ewo_corpus[ewo_corpus.word != \"\\n\"], ewo_fingerprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1029, 210) (1029,)\n",
      "(1029, 210) (1029,)\n"
     ]
    }
   ],
   "source": [
    "print(X_ewo.shape, ewo_target.shape)\n",
    "if len(X_ewo.shape) == len(X2.shape):\n",
    "    X_ewo = np.concatenate((X_ewo, X2))\n",
    "    ewo_target = np.concatenate((ewo_target, target2))\n",
    "    X_ewo, ewo_target = shuffle(X_ewo, ewo_target)\n",
    "print(X_ewo.shape, ewo_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1029, 105)\n"
     ]
    }
   ],
   "source": [
    "X_ewo_encoded = X_ewo.copy()\n",
    "if with_encoding:\n",
    "    X_ewo_encoded = encoder.predict(X_ewo)\n",
    "    print(X_ewo_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1029,) 1029\n"
     ]
    }
   ],
   "source": [
    "y_ewo = ewo_target.copy()\n",
    "print(y_ewo.shape, len(ewo_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 768,
     "status": "ok",
     "timestamp": 1514134592547,
     "user": {
      "displayName": "Michael Franklin Mbouopda",
      "photoUrl": "//lh3.googleusercontent.com/-2_Vuj9ESsJ0/AAAAAAAAAAI/AAAAAAAABSc/r-SRcN0jWPw/s50-c-k-no/photo.jpg",
      "userId": "117384351130599115261"
     },
     "user_tz": -60
    },
    "id": "fuSYSwUPDiQY",
    "outputId": "658c1a63-1a76-4487-e151-47364c6dbc85"
   },
   "outputs": [],
   "source": [
    "y_ewo = ewo_target.copy()\n",
    "y_ewo[:20]\n",
    "if not BINARY:\n",
    "    y_ewo = np_utils.to_categorical(y_ewo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6835,
     "status": "ok",
     "timestamp": 1514134601113,
     "user": {
      "displayName": "Michael Franklin Mbouopda",
      "photoUrl": "//lh3.googleusercontent.com/-2_Vuj9ESsJ0/AAAAAAAAAAI/AAAAAAAABSc/r-SRcN0jWPw/s50-c-k-no/photo.jpg",
      "userId": "117384351130599115261"
     },
     "user_tz": -60
    },
    "id": "Cev5j8YFzPYl",
    "outputId": "87ab5b4b-a75d-43b0-d346-5ba18a4522c0"
   },
   "outputs": [],
   "source": [
    "X_ewo_encoded = X_ewo_encoded.reshape((X_ewo.shape[0], timestep, X_ewo_encoded.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algoEval(X_train, y_train, X_val, y_val, X_ewo, y_ewo, epochs=epochs, model=None):\n",
    "    test_precision, train_precision, ewo_precision = [], [], []\n",
    "    test_recall, train_recall, ewo_recall = [], [], []\n",
    "    test_fscore, train_fscore, ewo_fscore = [], [], []\n",
    "    \n",
    "    test_result_by_tag = {}\n",
    "    train_result_by_tag = {}\n",
    "    ewo_result_by_tag = {}\n",
    "    for t in tagSet:\n",
    "        f1_key = \"F1-\"+t\n",
    "        p_key = \"P-\"+t\n",
    "        r_key = \"R-\"+t\n",
    "        train_result_by_tag[f1_key], train_result_by_tag[p_key], train_result_by_tag[r_key] = [], [], []\n",
    "        test_result_by_tag[f1_key], test_result_by_tag[p_key], test_result_by_tag[r_key] = [], [], []\n",
    "        ewo_result_by_tag[f1_key], ewo_result_by_tag[p_key], ewo_result_by_tag[r_key] = [], [], []\n",
    "\n",
    "    m, time_history = train_model(model, X_train, y_train, X_val, y_val, epochs=epochs)\n",
    "        \n",
    "    y_true, y_pred = predict(m, X_train, y_train)\n",
    "    p_train, r_train, f1_train = model_performance(y_true, y_pred)\n",
    "        \n",
    "    y_true_val, y_pred_val = predict(m, X_val, y_val)\n",
    "    p_val, r_val, f1_val = model_performance(y_true_val, y_pred_val)\n",
    "        \n",
    "    y_true_ewo, y_pred_ewo = predict(m, X_ewo, y_ewo) \n",
    "    p_ewo, r_ewo, f1_ewo = model_performance(y_true_ewo, y_pred_ewo)\n",
    "        \n",
    "    for t in range(len(int2tag)):\n",
    "        f1_key = \"F1-\" + int2tag[t]\n",
    "        p_key = \"P-\" + int2tag[t]\n",
    "        r_key = \"R-\" + int2tag[t]\n",
    "            \n",
    "        p, r, f1 = model_performace_by_tag(y_true, y_pred, t)\n",
    "        train_result_by_tag[p_key].append(p)\n",
    "        train_result_by_tag[r_key].append(r)\n",
    "        train_result_by_tag[f1_key].append(f1)\n",
    "            \n",
    "        p, r, f1 = model_performace_by_tag(y_true_val, y_pred_val, t)\n",
    "        test_result_by_tag[p_key].append(p)\n",
    "        test_result_by_tag[r_key].append(r)\n",
    "        test_result_by_tag[f1_key].append(f1)\n",
    "            \n",
    "        p, r, f1 = model_performace_by_tag(y_true_ewo, y_pred_ewo, t)\n",
    "        ewo_result_by_tag[p_key].append(p)\n",
    "        ewo_result_by_tag[r_key].append(r)\n",
    "        ewo_result_by_tag[f1_key].append(f1)\n",
    "                \n",
    "    test_precision.append(p_val)\n",
    "    train_precision.append(p_train)\n",
    "    ewo_precision.append(p_ewo)\n",
    "        \n",
    "    test_recall.append(r_val)\n",
    "    train_recall.append(r_train)\n",
    "    ewo_recall.append(r_ewo)\n",
    "        \n",
    "    test_fscore.append(f1_val)\n",
    "    train_fscore.append(f1_train)\n",
    "    ewo_fscore.append(f1_ewo)\n",
    "    return pd.DataFrame({\n",
    "        'P_test': test_precision, \n",
    "        'P_train': train_precision, \n",
    "        'P_ewo': ewo_precision, 'R_test': test_recall, 'R_train': train_recall, \n",
    "        'R_ewo': ewo_recall, 'F1-test': test_fscore, 'F1-train': train_fscore, 'F1-ewo': ewo_fscore}), pd.DataFrame(train_result_by_tag), pd.DataFrame(test_result_by_tag), pd.DataFrame(ewo_result_by_tag), time_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 611 samples, validate on 301 samples\n",
      "Epoch 1/50\n",
      "576/611 [===========================>..] - ETA: 0s - loss: 0.5489 - acc: 0.8802Epoch 00001: val_loss improved from inf to 0.53407, saving model to best-model-conll.hdfs\n",
      "611/611 [==============================] - 1s 913us/step - loss: 0.5367 - acc: 0.8822 - val_loss: 0.5341 - val_acc: 0.8870\n",
      "Epoch 2/50\n",
      "576/611 [===========================>..] - ETA: 0s - loss: 0.4757 - acc: 0.8802Epoch 00002: val_loss did not improve\n",
      "611/611 [==============================] - 1s 928us/step - loss: 0.4697 - acc: 0.8822 - val_loss: 0.5620 - val_acc: 0.8870\n",
      "Epoch 3/50\n",
      "576/611 [===========================>..] - ETA: 0s - loss: 0.4792 - acc: 0.8837Epoch 00003: val_loss improved from 0.53407 to 0.52882, saving model to best-model-conll.hdfs\n",
      "611/611 [==============================] - 0s 790us/step - loss: 0.4770 - acc: 0.8822 - val_loss: 0.5288 - val_acc: 0.8870\n",
      "Epoch 4/50\n",
      "608/611 [============================>.] - ETA: 0s - loss: 0.4716 - acc: 0.8816Epoch 00004: val_loss improved from 0.52882 to 0.51937, saving model to best-model-conll.hdfs\n",
      "611/611 [==============================] - 1s 1ms/step - loss: 0.4699 - acc: 0.8822 - val_loss: 0.5194 - val_acc: 0.8870\n",
      "Epoch 5/50\n",
      "576/611 [===========================>..] - ETA: 0s - loss: 0.4589 - acc: 0.8837Epoch 00005: val_loss did not improve\n",
      "611/611 [==============================] - 0s 712us/step - loss: 0.4635 - acc: 0.8822 - val_loss: 0.6283 - val_acc: 0.8870\n",
      "Epoch 6/50\n",
      "576/611 [===========================>..] - ETA: 0s - loss: 0.4658 - acc: 0.8819Epoch 00006: val_loss improved from 0.51937 to 0.50638, saving model to best-model-conll.hdfs\n",
      "611/611 [==============================] - 0s 782us/step - loss: 0.4627 - acc: 0.8822 - val_loss: 0.5064 - val_acc: 0.8870\n",
      "Epoch 7/50\n",
      "608/611 [============================>.] - ETA: 0s - loss: 0.4573 - acc: 0.885 - ETA: 0s - loss: 0.4536 - acc: 0.8832Epoch 00007: val_loss did not improve\n",
      "611/611 [==============================] - 0s 805us/step - loss: 0.4552 - acc: 0.8822 - val_loss: 0.7351 - val_acc: 0.8870\n",
      "Epoch 8/50\n",
      "576/611 [===========================>..] - ETA: 0s - loss: 0.4589 - acc: 0.8819Epoch 00008: val_loss did not improve\n",
      "611/611 [==============================] - 1s 971us/step - loss: 0.4542 - acc: 0.8822 - val_loss: 0.5066 - val_acc: 0.8870\n",
      "Epoch 9/50\n",
      "576/611 [===========================>..] - ETA: 0s - loss: 0.4601 - acc: 0.8802Epoch 00009: val_loss did not improve\n",
      "611/611 [==============================] - 0s 790us/step - loss: 0.4520 - acc: 0.8822 - val_loss: 0.5132 - val_acc: 0.8870\n",
      "Epoch 10/50\n",
      "576/611 [===========================>..] - ETA: 0s - loss: 0.4455 - acc: 0.8837Epoch 00010: val_loss improved from 0.50638 to 0.50542, saving model to best-model-conll.hdfs\n",
      "611/611 [==============================] - 1s 891us/step - loss: 0.4488 - acc: 0.8822 - val_loss: 0.5054 - val_acc: 0.8870\n",
      "Epoch 11/50\n",
      "576/611 [===========================>..] - ETA: 0s - loss: 0.4573 - acc: 0.8802Epoch 00011: val_loss did not improve\n",
      "611/611 [==============================] - 0s 707us/step - loss: 0.4521 - acc: 0.8822 - val_loss: 0.5058 - val_acc: 0.8870\n",
      "Epoch 12/50\n",
      "576/611 [===========================>..] - ETA: 0s - loss: 0.4524 - acc: 0.8819Epoch 00012: val_loss did not improve\n",
      "611/611 [==============================] - 1s 834us/step - loss: 0.4512 - acc: 0.8822 - val_loss: 0.5132 - val_acc: 0.8870\n",
      "Epoch 13/50\n",
      "512/611 [========================>.....] - ETA: 0s - loss: 0.4516 - acc: 0.8809Epoch 00013: val_loss did not improve\n",
      "611/611 [==============================] - 1s 835us/step - loss: 0.4507 - acc: 0.8822 - val_loss: 0.5081 - val_acc: 0.8870\n",
      "Epoch 14/50\n",
      "544/611 [=========================>....] - ETA: 0s - loss: 0.4350 - acc: 0.8879Epoch 00014: val_loss did not improve\n",
      "611/611 [==============================] - 1s 912us/step - loss: 0.4454 - acc: 0.8822 - val_loss: 0.5101 - val_acc: 0.8870\n",
      "Epoch 15/50\n",
      "544/611 [=========================>....] - ETA: 0s - loss: 0.4449 - acc: 0.8824Epoch 00015: val_loss did not improve\n",
      "611/611 [==============================] - 0s 775us/step - loss: 0.4468 - acc: 0.8822 - val_loss: 0.5108 - val_acc: 0.8870\n",
      "Epoch 16/50\n",
      "544/611 [=========================>....] - ETA: 0s - loss: 0.4596 - acc: 0.8787Epoch 00016: val_loss did not improve\n",
      "611/611 [==============================] - 0s 746us/step - loss: 0.4476 - acc: 0.8822 - val_loss: 0.5154 - val_acc: 0.8870\n",
      "Epoch 17/50\n",
      "608/611 [============================>.] - ETA: 0s - loss: 0.4507 - acc: 0.8816Epoch 00017: val_loss did not improve\n",
      "611/611 [==============================] - 1s 962us/step - loss: 0.4491 - acc: 0.8822 - val_loss: 0.5107 - val_acc: 0.8870\n",
      "Epoch 18/50\n",
      "576/611 [===========================>..] - ETA: 0s - loss: 0.4416 - acc: 0.8837Epoch 00018: val_loss did not improve\n",
      "611/611 [==============================] - 1s 1ms/step - loss: 0.4469 - acc: 0.8822 - val_loss: 0.5092 - val_acc: 0.8870\n",
      "Epoch 19/50\n",
      "608/611 [============================>.] - ETA: 0s - loss: 0.4482 - acc: 0.8816Epoch 00019: val_loss did not improve\n",
      "611/611 [==============================] - 0s 684us/step - loss: 0.4466 - acc: 0.8822 - val_loss: 0.5100 - val_acc: 0.8870\n",
      "Epoch 20/50\n",
      "544/611 [=========================>....] - ETA: 0s - loss: 0.4526 - acc: 0.8787Epoch 00020: val_loss did not improve\n",
      "611/611 [==============================] - 0s 762us/step - loss: 0.4462 - acc: 0.8822 - val_loss: 0.5128 - val_acc: 0.8870\n",
      "Epoch 21/50\n",
      "608/611 [============================>.] - ETA: 0s - loss: 0.4488 - acc: 0.8816Epoch 00021: val_loss did not improve\n",
      "611/611 [==============================] - 0s 673us/step - loss: 0.4472 - acc: 0.8822 - val_loss: 0.5094 - val_acc: 0.8870\n",
      "Epoch 22/50\n",
      "608/611 [============================>.] - ETA: 0s - loss: 0.4484 - acc: 0.8816Epoch 00022: val_loss did not improve\n",
      "611/611 [==============================] - 0s 679us/step - loss: 0.4469 - acc: 0.8822 - val_loss: 0.5092 - val_acc: 0.8870\n",
      "Epoch 23/50\n",
      "544/611 [=========================>....] - ETA: 0s - loss: 0.4479 - acc: 0.8824Epoch 00023: val_loss did not improve\n",
      "611/611 [==============================] - 0s 788us/step - loss: 0.4454 - acc: 0.8822 - val_loss: 0.5090 - val_acc: 0.8870\n",
      "Epoch 24/50\n",
      "544/611 [=========================>....] - ETA: 0s - loss: 0.4275 - acc: 0.8897Epoch 00024: val_loss did not improve\n",
      "611/611 [==============================] - 0s 795us/step - loss: 0.4466 - acc: 0.8822 - val_loss: 0.5098 - val_acc: 0.8870\n",
      "Epoch 25/50\n",
      "544/611 [=========================>....] - ETA: 0s - loss: 0.4448 - acc: 0.8805Epoch 00025: val_loss did not improve\n",
      "611/611 [==============================] - 1s 821us/step - loss: 0.4459 - acc: 0.8822 - val_loss: 0.5144 - val_acc: 0.8870\n",
      "Epoch 26/50\n",
      "608/611 [============================>.] - ETA: 0s - loss: 0.4430 - acc: 0.8832Epoch 00026: val_loss did not improve\n",
      "611/611 [==============================] - 0s 727us/step - loss: 0.4450 - acc: 0.8822 - val_loss: 0.5223 - val_acc: 0.8870\n",
      "Epoch 27/50\n",
      "512/611 [========================>.....] - ETA: 0s - loss: 0.4576 - acc: 0.8770Epoch 00027: val_loss did not improve\n",
      "611/611 [==============================] - 0s 717us/step - loss: 0.4474 - acc: 0.8822 - val_loss: 0.5077 - val_acc: 0.8870\n",
      "Epoch 28/50\n",
      "544/611 [=========================>....] - ETA: 0s - loss: 0.4392 - acc: 0.8842Epoch 00028: val_loss did not improve\n",
      "611/611 [==============================] - 0s 608us/step - loss: 0.4456 - acc: 0.8822 - val_loss: 0.5184 - val_acc: 0.8870\n",
      "Epoch 29/50\n",
      "608/611 [============================>.] - ETA: 0s - loss: 0.4478 - acc: 0.8816Epoch 00029: val_loss did not improve\n",
      "611/611 [==============================] - 1s 880us/step - loss: 0.4462 - acc: 0.8822 - val_loss: 0.5097 - val_acc: 0.8870\n",
      "Epoch 30/50\n",
      "544/611 [=========================>....] - ETA: 0s - loss: 0.4574 - acc: 0.8787Epoch 00030: val_loss did not improve\n",
      "611/611 [==============================] - 0s 701us/step - loss: 0.4458 - acc: 0.8822 - val_loss: 0.5112 - val_acc: 0.8870\n",
      "Epoch 31/50\n",
      "608/611 [============================>.] - ETA: 0s - loss: 0.4481 - acc: 0.8816Epoch 00031: val_loss did not improve\n",
      "611/611 [==============================] - 1s 832us/step - loss: 0.4465 - acc: 0.8822 - val_loss: 0.5122 - val_acc: 0.8870\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544/611 [=========================>....] - ETA: 0s - loss: 0.4634 - acc: 0.8768Epoch 00032: val_loss did not improve\n",
      "611/611 [==============================] - 0s 751us/step - loss: 0.4469 - acc: 0.8822 - val_loss: 0.5125 - val_acc: 0.8870\n",
      "Epoch 33/50\n",
      "576/611 [===========================>..] - ETA: 0s - loss: 0.4477 - acc: 0.8802Epoch 00033: val_loss did not improve\n",
      "611/611 [==============================] - 0s 723us/step - loss: 0.4472 - acc: 0.8822 - val_loss: 0.5111 - val_acc: 0.8870\n",
      "Epoch 34/50\n",
      "544/611 [=========================>....] - ETA: 0s - loss: 0.4511 - acc: 0.8824Epoch 00034: val_loss did not improve\n",
      "611/611 [==============================] - 1s 885us/step - loss: 0.4455 - acc: 0.8822 - val_loss: 0.5093 - val_acc: 0.8870\n",
      "Epoch 35/50\n",
      "608/611 [============================>.] - ETA: 0s - loss: 0.4466 - acc: 0.8816Epoch 00035: val_loss did not improve\n",
      "611/611 [==============================] - 0s 739us/step - loss: 0.4450 - acc: 0.8822 - val_loss: 0.5095 - val_acc: 0.8870\n",
      "Epoch 36/50\n",
      "608/611 [============================>.] - ETA: 0s - loss: 0.4469 - acc: 0.8816Epoch 00036: val_loss did not improve\n",
      "611/611 [==============================] - 1s 896us/step - loss: 0.4455 - acc: 0.8822 - val_loss: 0.5098 - val_acc: 0.8870\n",
      "Epoch 37/50\n",
      "608/611 [============================>.] - ETA: 0s - loss: 0.4460 - acc: 0.8816Epoch 00037: val_loss did not improve\n",
      "611/611 [==============================] - 1s 857us/step - loss: 0.4444 - acc: 0.8822 - val_loss: 0.5102 - val_acc: 0.8870\n",
      "Epoch 38/50\n",
      "576/611 [===========================>..] - ETA: 0s - loss: 0.4375 - acc: 0.8872Epoch 00038: val_loss did not improve\n",
      "611/611 [==============================] - 0s 703us/step - loss: 0.4460 - acc: 0.8822 - val_loss: 0.5116 - val_acc: 0.8870\n",
      "Epoch 39/50\n",
      "576/611 [===========================>..] - ETA: 0s - loss: 0.4443 - acc: 0.8837Epoch 00039: val_loss did not improve\n",
      "611/611 [==============================] - 0s 784us/step - loss: 0.4448 - acc: 0.8822 - val_loss: 0.5115 - val_acc: 0.8870\n",
      "Epoch 40/50\n",
      "544/611 [=========================>....] - ETA: 0s - loss: 0.4420 - acc: 0.8860Epoch 00040: val_loss did not improve\n",
      "611/611 [==============================] - 0s 707us/step - loss: 0.4477 - acc: 0.8822 - val_loss: 0.5171 - val_acc: 0.8870\n",
      "Epoch 41/50\n",
      "576/611 [===========================>..] - ETA: 0s - loss: 0.4515 - acc: 0.8802Epoch 00041: val_loss did not improve\n",
      "611/611 [==============================] - 0s 692us/step - loss: 0.4440 - acc: 0.8822 - val_loss: 0.5113 - val_acc: 0.8870\n",
      "Epoch 42/50\n",
      "576/611 [===========================>..] - ETA: 0s - loss: 0.4541 - acc: 0.8785Epoch 00042: val_loss did not improve\n",
      "611/611 [==============================] - 0s 729us/step - loss: 0.4450 - acc: 0.8822 - val_loss: 0.5117 - val_acc: 0.8870\n",
      "Epoch 43/50\n",
      "576/611 [===========================>..] - ETA: 0s - loss: 0.4551 - acc: 0.8802Epoch 00043: val_loss did not improve\n",
      "611/611 [==============================] - 0s 813us/step - loss: 0.4473 - acc: 0.8822 - val_loss: 0.5107 - val_acc: 0.8870\n",
      "Epoch 44/50\n",
      "576/611 [===========================>..] - ETA: 0s - loss: 0.4587 - acc: 0.8785Epoch 00044: val_loss did not improve\n",
      "611/611 [==============================] - 0s 693us/step - loss: 0.4470 - acc: 0.8822 - val_loss: 0.5101 - val_acc: 0.8870\n",
      "Epoch 45/50\n",
      "608/611 [============================>.] - ETA: 0s - loss: 0.4444 - acc: 0.8832Epoch 00045: val_loss did not improve\n",
      "611/611 [==============================] - 0s 717us/step - loss: 0.4467 - acc: 0.8822 - val_loss: 0.5149 - val_acc: 0.8870\n",
      "Epoch 46/50\n",
      "544/611 [=========================>....] - ETA: 0s - loss: 0.4502 - acc: 0.8805Epoch 00046: val_loss did not improve\n",
      "611/611 [==============================] - 0s 747us/step - loss: 0.4455 - acc: 0.8822 - val_loss: 0.5117 - val_acc: 0.8870\n",
      "Epoch 47/50\n",
      "512/611 [========================>.....] - ETA: 0s - loss: 0.4467 - acc: 0.8828Epoch 00047: val_loss did not improve\n",
      "611/611 [==============================] - 0s 730us/step - loss: 0.4476 - acc: 0.8822 - val_loss: 0.5210 - val_acc: 0.8870\n",
      "Epoch 48/50\n",
      "576/611 [===========================>..] - ETA: 0s - loss: 0.4461 - acc: 0.8837Epoch 00048: val_loss did not improve\n",
      "611/611 [==============================] - 0s 697us/step - loss: 0.4466 - acc: 0.8822 - val_loss: 0.5108 - val_acc: 0.8870\n",
      "Epoch 49/50\n",
      "576/611 [===========================>..] - ETA: 0s - loss: 0.4338 - acc: 0.8837Epoch 00049: val_loss did not improve\n",
      "611/611 [==============================] - 0s 725us/step - loss: 0.4469 - acc: 0.8822 - val_loss: 0.5112 - val_acc: 0.8870\n",
      "Epoch 50/50\n",
      "608/611 [============================>.] - ETA: 0s - loss: 0.4422 - acc: 0.8832Epoch 00050: val_loss did not improve\n",
      "611/611 [==============================] - 0s 789us/step - loss: 0.4469 - acc: 0.8822 - val_loss: 0.5132 - val_acc: 0.8870\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n"
     ]
    }
   ],
   "source": [
    "model = create_model(X_encoded.shape[1], len(tagSet))\n",
    "if with_encoding:\n",
    "    resultEval, train_by_tag, test_by_tag, ewo_by_tag, time_history_autoencoder = algoEval(X_train, y_train, X_val, y_val, X_ewo_encoded, y_ewo, model=model, epochs=50)\n",
    "    pd.DataFrame(time_history_autoencoder.times).to_csv(\"time_history_autoencoder.csv\")\n",
    "else:\n",
    "    resultEval, train_by_tag, test_by_tag, ewo_by_tag, time_history_default = algoEval(X_train, y_train, X_val, y_val, X_ewo_encoded, y_ewo, model=model, epochs=50)\n",
    "    pd.DataFrame(time_history_default.times).to_csv(\"time_history_default.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    time_history_default = pd.read_csv(\"time_history_default.csv\", usecols=[1])[\"0\"].values.tolist()\n",
    "    time_history_autoencoder = pd.read_csv(\"time_history_autoencoder.csv\", usecols=[1])[\"0\"].values.tolist()\n",
    "    figure, axis = plt.subplots(figsize=(20, 4))\n",
    "    axis.plot(time_history_default, label=\"default\")\n",
    "    axis.plot(time_history_autoencoder, ls=\"--\", label=\"autoencoder\")\n",
    "    axis.set_ylabel(\"Training time\")\n",
    "    axis.set_xlabel(\"Epochs\")\n",
    "    axis.legend()\n",
    "    figure.savefig(fname=\"time-by-epochs.png\")\n",
    "except FileNotFoundError as e:\n",
    "    print(\"file not found\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_b`y_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_by_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ewo_by_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultEval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultEval.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algoCrossVal(model, X, y, X_ewo, y_ewo, k = 10, repeat=1): \n",
    "    block_size = int(X.shape[0] / 4)   \n",
    "    output = None\n",
    "    train_by_tags, test_by_tags, ewo_by_tags = None, None, None\n",
    "    for it in range(repeat):\n",
    "        print(\"AlgoCrossValIter -\", it+1)\n",
    "        model = create_model(X.shape[1], len(tagSet))\n",
    "        results = None\n",
    "        train_by_tagsTmp, test_by_tagsTmp, ewo_by_tagsTmp = None, None, None\n",
    "        for i in range(k):\n",
    "            X_val, y_val = X[i*block_size:i*block_size+block_size], y[i*block_size:i*block_size+block_size]\n",
    "            X_train = np.concatenate((X[0:i*block_size], X[i*block_size+block_size:]))\n",
    "            y_train = np.concatenate((y[0:i*block_size], y[i*block_size+block_size:]))\n",
    "\n",
    "            X_train = X_train.reshape(X_train.shape[0], timestep, X_train.shape[1])\n",
    "            X_val = X_val.reshape(X_val.shape[0], timestep, X_val.shape[1])\n",
    "\n",
    "            result, train_by_tag, test_by_tag, ewo_by_tag, thistory = algoEval(X_train, y_train, X_val, y_val, X_ewo, y_ewo, model=model)\n",
    "            if results is None:\n",
    "                results = result.copy()\n",
    "                train_by_tagsTmp, test_by_tagsTmp, ewo_by_tagsTmp = train_by_tag.copy(), test_by_tag.copy(), ewo_by_tag.copy()\n",
    "            else:\n",
    "                results = pd.concat([results, result], ignore_index=True)\n",
    "                train_by_tagsTmp = pd.concat([train_by_tagsTmp, train_by_tag], ignore_index=True)\n",
    "                test_by_tagsTmp = pd.concat([test_by_tagsTmp, test_by_tag], ignore_index=True)\n",
    "                ewo_by_tagsTmp = pd.concat([ewo_by_tagsTmp, ewo_by_tag], ignore_index=True)\n",
    "        \n",
    "        if output is None:\n",
    "            output = results.mean(axis=0).to_frame()\n",
    "            train_by_tags = train_by_tagsTmp.mean(axis=0).to_frame()\n",
    "            test_by_tags = test_by_tagsTmp.mean(axis=0).to_frame()\n",
    "            ewo_by_tags = ewo_by_tagsTmp.mean(axis=0).to_frame()\n",
    "        else:\n",
    "            output = pd.concat([output, results.mean(axis=0).to_frame()], axis=1)\n",
    "            train_by_tags = pd.concat([train_by_tags, train_by_tagsTmp.mean(axis=0).to_frame()], axis=1)\n",
    "            test_by_tags = pd.concat([test_by_tags, test_by_tagsTmp.mean(axis=0).to_frame()], axis=1)\n",
    "            ewo_by_tags = pd.concat([ewo_by_tags, ewo_by_tagsTmp.mean(axis=0).to_frame()], axis=1)\n",
    "\n",
    "    return output, train_by_tags, test_by_tags, ewo_by_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlgoCrossValIter - 1\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.5719 - acc: 0.8455Epoch 00001: val_loss improved from inf to 0.48499, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 895us/step - loss: 0.5332 - acc: 0.8582 - val_loss: 0.4850 - val_acc: 0.8816\n",
      "Epoch 2/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4579 - acc: 0.8875Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 1s 926us/step - loss: 0.4727 - acc: 0.8845 - val_loss: 0.5111 - val_acc: 0.8816\n",
      "Epoch 3/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4775 - acc: 0.8839Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 0s 653us/step - loss: 0.4770 - acc: 0.8845 - val_loss: 0.4965 - val_acc: 0.8816\n",
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4605 - acc: 0.8884Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 0s 654us/step - loss: 0.4750 - acc: 0.8845 - val_loss: 0.5622 - val_acc: 0.8816\n",
      "Epoch 5/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4846 - acc: 0.8783Epoch 00005: val_loss improved from 0.48499 to 0.46754, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 652us/step - loss: 0.4644 - acc: 0.8845 - val_loss: 0.4675 - val_acc: 0.8816\n",
      "Epoch 6/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4627 - acc: 0.8839Epoch 00006: val_loss improved from 0.46754 to 0.46696, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 752us/step - loss: 0.4601 - acc: 0.8845 - val_loss: 0.4670 - val_acc: 0.8816\n",
      "Epoch 7/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4740 - acc: 0.8797Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 1s 918us/step - loss: 0.4597 - acc: 0.8845 - val_loss: 0.4898 - val_acc: 0.8816\n",
      "Epoch 8/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4437 - acc: 0.8884Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 1s 920us/step - loss: 0.4620 - acc: 0.8845 - val_loss: 0.4705 - val_acc: 0.8816\n",
      "Epoch 9/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4547 - acc: 0.8839Epoch 00009: val_loss improved from 0.46696 to 0.46100, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 871us/step - loss: 0.4560 - acc: 0.8845 - val_loss: 0.4610 - val_acc: 0.8816\n",
      "Epoch 10/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4774 - acc: 0.8785Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 573us/step - loss: 0.4559 - acc: 0.8845 - val_loss: 0.4619 - val_acc: 0.8816\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4494 - acc: 0.8869Epoch 00001: val_loss improved from inf to 0.48033, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 678us/step - loss: 0.4475 - acc: 0.8874 - val_loss: 0.4803 - val_acc: 0.8728\n",
      "Epoch 2/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4266 - acc: 0.8931Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 0s 687us/step - loss: 0.4485 - acc: 0.8874 - val_loss: 0.4810 - val_acc: 0.8728\n",
      "Epoch 3/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4588 - acc: 0.8832Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 0s 709us/step - loss: 0.4492 - acc: 0.8874 - val_loss: 0.4882 - val_acc: 0.8728\n",
      "Epoch 4/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4498 - acc: 0.8875Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 1s 761us/step - loss: 0.4487 - acc: 0.8874 - val_loss: 0.4843 - val_acc: 0.8728\n",
      "Epoch 5/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4531 - acc: 0.8859Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 666us/step - loss: 0.4482 - acc: 0.8874 - val_loss: 0.4936 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4481 - acc: 0.8884- ETA: 0s - loss: 0.4214 - acc: 0.8Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 685us/step - loss: 0.4498 - acc: 0.8874 - val_loss: 0.4826 - val_acc: 0.8728\n",
      "Epoch 7/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4413 - acc: 0.8914Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 624us/step - loss: 0.4496 - acc: 0.8874 - val_loss: 0.4807 - val_acc: 0.8728\n",
      "Epoch 8/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4685 - acc: 0.8797Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 697us/step - loss: 0.4447 - acc: 0.8874 - val_loss: 0.5003 - val_acc: 0.8728\n",
      "Epoch 9/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4553 - acc: 0.8854Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 518us/step - loss: 0.4492 - acc: 0.8874 - val_loss: 0.4898 - val_acc: 0.8728\n",
      "Epoch 10/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4545 - acc: 0.8854Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 515us/step - loss: 0.4484 - acc: 0.8874 - val_loss: 0.4905 - val_acc: 0.8728\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4958 - acc: 0.8698Epoch 00001: val_loss improved from inf to 0.39228, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 631us/step - loss: 0.4799 - acc: 0.8757 - val_loss: 0.3923 - val_acc: 0.9079\n",
      "Epoch 2/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4847 - acc: 0.8733Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 1s 785us/step - loss: 0.4766 - acc: 0.8757 - val_loss: 0.3925 - val_acc: 0.9079\n",
      "Epoch 3/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4706 - acc: 0.8780Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 1s 780us/step - loss: 0.4776 - acc: 0.8757 - val_loss: 0.4099 - val_acc: 0.9079\n",
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4623 - acc: 0.8795Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 1s 740us/step - loss: 0.4753 - acc: 0.8757 - val_loss: 0.4124 - val_acc: 0.9079\n",
      "Epoch 5/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4782 - acc: 0.8734Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 721us/step - loss: 0.4746 - acc: 0.8757 - val_loss: 0.4046 - val_acc: 0.9079\n",
      "Epoch 6/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4622 - acc: 0.8797Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 1s 735us/step - loss: 0.4774 - acc: 0.8757 - val_loss: 0.4140 - val_acc: 0.9079\n",
      "Epoch 7/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4766 - acc: 0.8750Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 1s 949us/step - loss: 0.4766 - acc: 0.8757 - val_loss: 0.4134 - val_acc: 0.9079\n",
      "Epoch 8/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4883 - acc: 0.8734Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 577us/step - loss: 0.4766 - acc: 0.8757 - val_loss: 0.4207 - val_acc: 0.9079\n",
      "Epoch 9/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4864 - acc: 0.8719Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 1s 782us/step - loss: 0.4746 - acc: 0.8757 - val_loss: 0.4214 - val_acc: 0.9079\n",
      "Epoch 10/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4798 - acc: 0.8750Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 1s 903us/step - loss: 0.4770 - acc: 0.8757 - val_loss: 0.4225 - val_acc: 0.9079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4602 - acc: 0.8875Epoch 00001: val_loss improved from inf to 0.47975, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 865us/step - loss: 0.4587 - acc: 0.8874 - val_loss: 0.4798 - val_acc: 0.8728\n",
      "Epoch 2/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4367 - acc: 0.8906Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 0s 635us/step - loss: 0.4586 - acc: 0.8874 - val_loss: 0.4804 - val_acc: 0.8728\n",
      "Epoch 3/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4873 - acc: 0.8783Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 1s 758us/step - loss: 0.4562 - acc: 0.8874 - val_loss: 0.4948 - val_acc: 0.8728\n",
      "Epoch 4/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4780 - acc: 0.8816Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 1s 836us/step - loss: 0.4582 - acc: 0.8874 - val_loss: 0.4837 - val_acc: 0.8728\n",
      "Epoch 5/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4617 - acc: 0.8854Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 1s 761us/step - loss: 0.4559 - acc: 0.8874 - val_loss: 0.4823 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4584 - acc: 0.8869Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 653us/step - loss: 0.4559 - acc: 0.8874 - val_loss: 0.4830 - val_acc: 0.8728\n",
      "Epoch 7/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4538 - acc: 0.8884Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 692us/step - loss: 0.4547 - acc: 0.8874 - val_loss: 0.4811 - val_acc: 0.8728\n",
      "Epoch 8/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4614 - acc: 0.8854Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 720us/step - loss: 0.4557 - acc: 0.8874 - val_loss: 0.4837 - val_acc: 0.8728\n",
      "Epoch 9/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4530 - acc: 0.8898Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 585us/step - loss: 0.4537 - acc: 0.8874 - val_loss: 0.4819 - val_acc: 0.8728\n",
      "Epoch 10/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4520 - acc: 0.8891Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 1s 846us/step - loss: 0.4537 - acc: 0.8874 - val_loss: 0.4798 - val_acc: 0.8728\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 555us/step - loss: 0.4607 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "224/912 [======>.......................] - ETA: 0s - loss: 0.4258 - acc: 0.8839"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/keras/callbacks.py:403: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912/912 [==============================] - 1s 658us/step - loss: 0.4608 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 721us/step - loss: 0.4615 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 537us/step - loss: 0.4607 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 561us/step - loss: 0.4604 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 618us/step - loss: 0.4596 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 584us/step - loss: 0.4600 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 633us/step - loss: 0.4613 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 571us/step - loss: 0.4594 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 611us/step - loss: 0.4596 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 653us/step - loss: 0.4588 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 612us/step - loss: 0.4588 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 684us/step - loss: 0.4577 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 702us/step - loss: 0.4595 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 855us/step - loss: 0.4582 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 838us/step - loss: 0.4586 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 695us/step - loss: 0.4595 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 685us/step - loss: 0.4580 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 647us/step - loss: 0.4571 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 784us/step - loss: 0.4575 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 666us/step - loss: 0.4586 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 775us/step - loss: 0.4580 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 906us/step - loss: 0.4572 - acc: 0.8838 0s - loss: 0.4901 - acc: \n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 725us/step - loss: 0.4568 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 610us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 679us/step - loss: 0.4578 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 614us/step - loss: 0.4558 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 754us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 743us/step - loss: 0.4581 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 669us/step - loss: 0.4570 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 715us/step - loss: 0.4578 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 689us/step - loss: 0.4568 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 716us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 750us/step - loss: 0.4542 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 790us/step - loss: 0.4589 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 624us/step - loss: 0.4586 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 573us/step - loss: 0.4576 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 687us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 695us/step - loss: 0.4571 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 561us/step - loss: 0.4573 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 814us/step - loss: 0.4580 - acc: 0.8838 0s - loss: 0.4442 - acc: 0\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 593us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 645us/step - loss: 0.4581 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 772us/step - loss: 0.4559 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 751us/step - loss: 0.4598 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 616us/step - loss: 0.4569 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 602us/step - loss: 0.4565 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 575us/step - loss: 0.4556 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 619us/step - loss: 0.4580 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 551us/step - loss: 0.4526 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 796us/step - loss: 0.4576 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 583us/step - loss: 0.4569 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 676us/step - loss: 0.4588 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 565us/step - loss: 0.4572 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 650us/step - loss: 0.4562 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 618us/step - loss: 0.4580 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 709us/step - loss: 0.4571 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 556us/step - loss: 0.4560 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 0s 543us/step - loss: 0.4561 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 545us/step - loss: 0.4571 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "AlgoCrossValIter - 2\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.5281 - acc: 0.8351Epoch 00001: val_loss improved from inf to 0.48919, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 886us/step - loss: 0.5184 - acc: 0.8450 - val_loss: 0.4892 - val_acc: 0.8816\n",
      "Epoch 2/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4875 - acc: 0.8839Epoch 00002: val_loss improved from 0.48919 to 0.46894, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 735us/step - loss: 0.4847 - acc: 0.8845 - val_loss: 0.4689 - val_acc: 0.8816\n",
      "Epoch 3/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4508 - acc: 0.8854Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 0s 598us/step - loss: 0.4571 - acc: 0.8845 - val_loss: 0.4874 - val_acc: 0.8816\n",
      "Epoch 4/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4529 - acc: 0.8882Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 0s 596us/step - loss: 0.4662 - acc: 0.8845 - val_loss: 0.5625 - val_acc: 0.8816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4614 - acc: 0.8859Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 710us/step - loss: 0.4705 - acc: 0.8845 - val_loss: 0.4794 - val_acc: 0.8816\n",
      "Epoch 6/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4674 - acc: 0.8854Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 622us/step - loss: 0.4648 - acc: 0.8845 - val_loss: 0.4701 - val_acc: 0.8816\n",
      "Epoch 7/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4647 - acc: 0.8824Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 1s 818us/step - loss: 0.4588 - acc: 0.8845 - val_loss: 0.4783 - val_acc: 0.8816\n",
      "Epoch 8/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4686 - acc: 0.8824Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 726us/step - loss: 0.4626 - acc: 0.8845 - val_loss: 0.4714 - val_acc: 0.8816\n",
      "Epoch 9/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4520 - acc: 0.8872Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 584us/step - loss: 0.4558 - acc: 0.8845 - val_loss: 0.4734 - val_acc: 0.8816\n",
      "Epoch 10/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4620 - acc: 0.8832Epoch 00010: val_loss improved from 0.46894 to 0.46298, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 771us/step - loss: 0.4581 - acc: 0.8845 - val_loss: 0.4630 - val_acc: 0.8816\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4714 - acc: 0.8816Epoch 00001: val_loss improved from inf to 0.49392, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 793us/step - loss: 0.4517 - acc: 0.8874 - val_loss: 0.4939 - val_acc: 0.8728\n",
      "Epoch 2/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4248 - acc: 0.8976Epoch 00002: val_loss improved from 0.49392 to 0.48614, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 949us/step - loss: 0.4500 - acc: 0.8874 - val_loss: 0.4861 - val_acc: 0.8728\n",
      "Epoch 3/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4675 - acc: 0.8812Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 1s 802us/step - loss: 0.4478 - acc: 0.8874 - val_loss: 0.4978 - val_acc: 0.8728\n",
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4573 - acc: 0.8854Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 1s 749us/step - loss: 0.4514 - acc: 0.8874 - val_loss: 0.4883 - val_acc: 0.8728\n",
      "Epoch 5/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4523 - acc: 0.8859Epoch 00005: val_loss improved from 0.48614 to 0.47884, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 731us/step - loss: 0.4473 - acc: 0.8874 - val_loss: 0.4788 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4459 - acc: 0.8891Epoch 00006: val_loss improved from 0.47884 to 0.47826, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 802us/step - loss: 0.4481 - acc: 0.8874 - val_loss: 0.4783 - val_acc: 0.8728\n",
      "Epoch 7/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4564 - acc: 0.8844Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 1s 753us/step - loss: 0.4477 - acc: 0.8874 - val_loss: 0.4879 - val_acc: 0.8728\n",
      "Epoch 8/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4442 - acc: 0.8899Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 601us/step - loss: 0.4495 - acc: 0.8874 - val_loss: 0.4812 - val_acc: 0.8728\n",
      "Epoch 9/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4526 - acc: 0.8854Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 540us/step - loss: 0.4464 - acc: 0.8874 - val_loss: 0.4951 - val_acc: 0.8728\n",
      "Epoch 10/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4471 - acc: 0.8884Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 658us/step - loss: 0.4482 - acc: 0.8874 - val_loss: 0.4803 - val_acc: 0.8728\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4758 - acc: 0.8766Epoch 00001: val_loss improved from inf to 0.39973, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 643us/step - loss: 0.4785 - acc: 0.8757 - val_loss: 0.3997 - val_acc: 0.9079\n",
      "Epoch 2/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4666 - acc: 0.8781Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 1s 790us/step - loss: 0.4769 - acc: 0.8757 - val_loss: 0.4077 - val_acc: 0.9079\n",
      "Epoch 3/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4752 - acc: 0.8766Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 0s 643us/step - loss: 0.4781 - acc: 0.8757 - val_loss: 0.4041 - val_acc: 0.9079\n",
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4679 - acc: 0.8780Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 0s 719us/step - loss: 0.4742 - acc: 0.8757 - val_loss: 0.4342 - val_acc: 0.9079\n",
      "Epoch 5/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4850 - acc: 0.8735Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 602us/step - loss: 0.4787 - acc: 0.8757 - val_loss: 0.4029 - val_acc: 0.9079\n",
      "Epoch 6/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4737 - acc: 0.8783Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 712us/step - loss: 0.4770 - acc: 0.8757 - val_loss: 0.4128 - val_acc: 0.9079\n",
      "Epoch 7/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4845 - acc: 0.8733Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 608us/step - loss: 0.4762 - acc: 0.8757 - val_loss: 0.4178 - val_acc: 0.9079\n",
      "Epoch 8/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4960 - acc: 0.8701Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 1s 747us/step - loss: 0.4763 - acc: 0.8757 - val_loss: 0.4167 - val_acc: 0.9079\n",
      "Epoch 9/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4792 - acc: 0.8750Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 665us/step - loss: 0.4766 - acc: 0.8757 - val_loss: 0.4224 - val_acc: 0.9079\n",
      "Epoch 10/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4771 - acc: 0.8750Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 660us/step - loss: 0.4761 - acc: 0.8757 - val_loss: 0.4233 - val_acc: 0.9079\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4600 - acc: 0.8869Epoch 00001: val_loss improved from inf to 0.48606, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 833us/step - loss: 0.4574 - acc: 0.8874 - val_loss: 0.4861 - val_acc: 0.8728\n",
      "Epoch 2/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4618 - acc: 0.8869Epoch 00002: val_loss improved from 0.48606 to 0.47976, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 865us/step - loss: 0.4593 - acc: 0.8874 - val_loss: 0.4798 - val_acc: 0.8728\n",
      "Epoch 3/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4648 - acc: 0.8854Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 0s 706us/step - loss: 0.4586 - acc: 0.8874 - val_loss: 0.4864 - val_acc: 0.8728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4556 - acc: 0.8884Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 1s 778us/step - loss: 0.4589 - acc: 0.8874 - val_loss: 0.4799 - val_acc: 0.8728\n",
      "Epoch 5/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4574 - acc: 0.8898Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 696us/step - loss: 0.4576 - acc: 0.8874 - val_loss: 0.4811 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4643 - acc: 0.8859Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 1s 881us/step - loss: 0.4557 - acc: 0.8874 - val_loss: 0.4850 - val_acc: 0.8728\n",
      "Epoch 7/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4536 - acc: 0.8884Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 672us/step - loss: 0.4569 - acc: 0.8874 - val_loss: 0.4816 - val_acc: 0.8728\n",
      "Epoch 8/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4649 - acc: 0.8844Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 1s 853us/step - loss: 0.4583 - acc: 0.8874 - val_loss: 0.4842 - val_acc: 0.8728\n",
      "Epoch 9/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4600 - acc: 0.8869Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 649us/step - loss: 0.4574 - acc: 0.8874 - val_loss: 0.4811 - val_acc: 0.8728\n",
      "Epoch 10/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4585 - acc: 0.8869Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 1s 857us/step - loss: 0.4559 - acc: 0.8874 - val_loss: 0.4834 - val_acc: 0.8728\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 723us/step - loss: 0.4636 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 576us/step - loss: 0.4617 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 649us/step - loss: 0.4601 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 663us/step - loss: 0.4605 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 526us/step - loss: 0.4623 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 0s 546us/step - loss: 0.4623 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 568us/step - loss: 0.4606 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 657us/step - loss: 0.4594 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 614us/step - loss: 0.4613 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 816us/step - loss: 0.4613 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 569us/step - loss: 0.4602 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 655us/step - loss: 0.4588 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 657us/step - loss: 0.4585 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 555us/step - loss: 0.4582 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 619us/step - loss: 0.4584 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 688us/step - loss: 0.4570 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 0s 546us/step - loss: 0.4594 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - ETA: 0s - loss: 0.4538 - acc: 0.885 - 0s 515us/step - loss: 0.4583 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 0s 477us/step - loss: 0.4587 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 599us/step - loss: 0.4583 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 561us/step - loss: 0.4570 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 579us/step - loss: 0.4586 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 0s 515us/step - loss: 0.4577 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 580us/step - loss: 0.4564 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 516us/step - loss: 0.4585 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 0s 459us/step - loss: 0.4580 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 0s 513us/step - loss: 0.4587 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 0s 501us/step - loss: 0.4591 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 0s 525us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 570us/step - loss: 0.4558 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 0s 541us/step - loss: 0.4598 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 0s 505us/step - loss: 0.4576 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 697us/step - loss: 0.4577 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 719us/step - loss: 0.4583 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 811us/step - loss: 0.4561 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 0s 532us/step - loss: 0.4589 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 0s 484us/step - loss: 0.4579 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 567us/step - loss: 0.4544 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 0s 451us/step - loss: 0.4588 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 468us/step - loss: 0.4573 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 582us/step - loss: 0.4564 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 0s 485us/step - loss: 0.4585 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 0s 456us/step - loss: 0.4579 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 470us/step - loss: 0.4578 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 494us/step - loss: 0.4564 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 559us/step - loss: 0.4560 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 625us/step - loss: 0.4574 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 0s 488us/step - loss: 0.4569 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 0s 530us/step - loss: 0.4572 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 529us/step - loss: 0.4577 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 0s 497us/step - loss: 0.4579 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 0s 500us/step - loss: 0.4545 - acc: 0.8838\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912/912 [==============================] - 1s 610us/step - loss: 0.4560 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 638us/step - loss: 0.4575 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 700us/step - loss: 0.4563 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 652us/step - loss: 0.4541 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 725us/step - loss: 0.4578 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 0s 537us/step - loss: 0.4575 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 807us/step - loss: 0.4576 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 641us/step - loss: 0.4572 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "AlgoCrossValIter - 3\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.5714 - acc: 0.8408Epoch 00001: val_loss improved from inf to 0.46805, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 902us/step - loss: 0.5670 - acc: 0.8421 - val_loss: 0.4680 - val_acc: 0.8816\n",
      "Epoch 2/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4800 - acc: 0.8865Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 0s 664us/step - loss: 0.4863 - acc: 0.8845 - val_loss: 0.5827 - val_acc: 0.8816\n",
      "Epoch 3/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4744 - acc: 0.8839Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 0s 654us/step - loss: 0.4715 - acc: 0.8845 - val_loss: 0.4764 - val_acc: 0.8816\n",
      "Epoch 4/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4738 - acc: 0.8828Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 0s 639us/step - loss: 0.4713 - acc: 0.8845 - val_loss: 0.4694 - val_acc: 0.8816\n",
      "Epoch 5/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4716 - acc: 0.8844Epoch 00005: val_loss improved from 0.46805 to 0.46622, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 590us/step - loss: 0.4724 - acc: 0.8845 - val_loss: 0.4662 - val_acc: 0.8816\n",
      "Epoch 6/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4786 - acc: 0.8837Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 590us/step - loss: 0.4696 - acc: 0.8845 - val_loss: 0.4705 - val_acc: 0.8816\n",
      "Epoch 7/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4647 - acc: 0.8844Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 721us/step - loss: 0.4699 - acc: 0.8845 - val_loss: 0.4870 - val_acc: 0.8816\n",
      "Epoch 8/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4566 - acc: 0.8849Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 1s 760us/step - loss: 0.4644 - acc: 0.8845 - val_loss: 0.4834 - val_acc: 0.8816\n",
      "Epoch 9/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4661 - acc: 0.8832Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 1s 776us/step - loss: 0.4616 - acc: 0.8845 - val_loss: 0.4832 - val_acc: 0.8816\n",
      "Epoch 10/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4581 - acc: 0.8865Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 691us/step - loss: 0.4632 - acc: 0.8845 - val_loss: 0.4808 - val_acc: 0.8816\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4515 - acc: 0.8882Epoch 00001: val_loss improved from inf to 0.49255, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 753us/step - loss: 0.4478 - acc: 0.8874 - val_loss: 0.4926 - val_acc: 0.8728\n",
      "Epoch 2/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4498 - acc: 0.8875Epoch 00002: val_loss improved from 0.49255 to 0.48791, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 784us/step - loss: 0.4525 - acc: 0.8874 - val_loss: 0.4879 - val_acc: 0.8728\n",
      "Epoch 3/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4614 - acc: 0.8859Epoch 00003: val_loss improved from 0.48791 to 0.48660, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 748us/step - loss: 0.4535 - acc: 0.8874 - val_loss: 0.4866 - val_acc: 0.8728\n",
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4557 - acc: 0.8854Epoch 00004: val_loss improved from 0.48660 to 0.48270, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 734us/step - loss: 0.4506 - acc: 0.8874 - val_loss: 0.4827 - val_acc: 0.8728\n",
      "Epoch 5/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4466 - acc: 0.8875Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 598us/step - loss: 0.4487 - acc: 0.8874 - val_loss: 0.4851 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4548 - acc: 0.8859Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 589us/step - loss: 0.4499 - acc: 0.8874 - val_loss: 0.4838 - val_acc: 0.8728\n",
      "Epoch 7/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4489 - acc: 0.8884Epoch 00007: val_loss improved from 0.48270 to 0.48175, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 790us/step - loss: 0.4503 - acc: 0.8874 - val_loss: 0.4817 - val_acc: 0.8728\n",
      "Epoch 8/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4475 - acc: 0.8882Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 721us/step - loss: 0.4479 - acc: 0.8874 - val_loss: 0.4820 - val_acc: 0.8728\n",
      "Epoch 9/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4344 - acc: 0.8922Epoch 00009: val_loss improved from 0.48175 to 0.47930, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 820us/step - loss: 0.4483 - acc: 0.8874 - val_loss: 0.4793 - val_acc: 0.8728\n",
      "Epoch 10/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4535 - acc: 0.8854Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 713us/step - loss: 0.4480 - acc: 0.8874 - val_loss: 0.4814 - val_acc: 0.8728\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4643 - acc: 0.8810Epoch 00001: val_loss improved from inf to 0.39796, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 630us/step - loss: 0.4769 - acc: 0.8757 - val_loss: 0.3980 - val_acc: 0.9079\n",
      "Epoch 2/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4807 - acc: 0.8733Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 0s 594us/step - loss: 0.4763 - acc: 0.8757 - val_loss: 0.4018 - val_acc: 0.9079\n",
      "Epoch 3/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4647 - acc: 0.8795Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 0s 687us/step - loss: 0.4758 - acc: 0.8757 - val_loss: 0.4058 - val_acc: 0.9079\n",
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4761 - acc: 0.8765Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 0s 537us/step - loss: 0.4768 - acc: 0.8757 - val_loss: 0.4024 - val_acc: 0.9079\n",
      "Epoch 5/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4731 - acc: 0.8765Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 521us/step - loss: 0.4768 - acc: 0.8757 - val_loss: 0.4032 - val_acc: 0.9079\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4918 - acc: 0.8703Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 687us/step - loss: 0.4753 - acc: 0.8757 - val_loss: 0.4042 - val_acc: 0.9079\n",
      "Epoch 7/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4689 - acc: 0.8780Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 1s 900us/step - loss: 0.4756 - acc: 0.8757 - val_loss: 0.4248 - val_acc: 0.9079\n",
      "Epoch 8/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4642 - acc: 0.8781Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 1s 921us/step - loss: 0.4757 - acc: 0.8757 - val_loss: 0.4293 - val_acc: 0.9079\n",
      "Epoch 9/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4772 - acc: 0.8750Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 1s 799us/step - loss: 0.4768 - acc: 0.8757 - val_loss: 0.4159 - val_acc: 0.9079\n",
      "Epoch 10/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4822 - acc: 0.8735Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 674us/step - loss: 0.4758 - acc: 0.8757 - val_loss: 0.4183 - val_acc: 0.9079\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4122 - acc: 0.8938Epoch 00001: val_loss improved from inf to 0.48094, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 705us/step - loss: 0.4570 - acc: 0.8874 - val_loss: 0.4809 - val_acc: 0.8728\n",
      "Epoch 2/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4569 - acc: 0.8869Epoch 00002: val_loss improved from 0.48094 to 0.48003, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 754us/step - loss: 0.4577 - acc: 0.8874 - val_loss: 0.4800 - val_acc: 0.8728\n",
      "Epoch 3/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4584 - acc: 0.8869Epoch 00003: val_loss improved from 0.48003 to 0.47944, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 779us/step - loss: 0.4561 - acc: 0.8874 - val_loss: 0.4794 - val_acc: 0.8728\n",
      "Epoch 4/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4725 - acc: 0.8828Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 1s 844us/step - loss: 0.4571 - acc: 0.8874 - val_loss: 0.4843 - val_acc: 0.8728\n",
      "Epoch 5/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4532 - acc: 0.8875Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 717us/step - loss: 0.4575 - acc: 0.8874 - val_loss: 0.4819 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4315 - acc: 0.8898Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 655us/step - loss: 0.4556 - acc: 0.8874 - val_loss: 0.4807 - val_acc: 0.8728\n",
      "Epoch 7/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4535 - acc: 0.8884Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 676us/step - loss: 0.4545 - acc: 0.8874 - val_loss: 0.4806 - val_acc: 0.8728\n",
      "Epoch 8/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4336 - acc: 0.8882Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 1s 738us/step - loss: 0.4562 - acc: 0.8874 - val_loss: 0.4804 - val_acc: 0.8728\n",
      "Epoch 9/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4568 - acc: 0.8869Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 588us/step - loss: 0.4543 - acc: 0.8874 - val_loss: 0.4812 - val_acc: 0.8728\n",
      "Epoch 10/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4678 - acc: 0.8832Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 1s 765us/step - loss: 0.4552 - acc: 0.8874 - val_loss: 0.4836 - val_acc: 0.8728\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 599us/step - loss: 0.4615 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 0s 494us/step - loss: 0.4611 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 0s 531us/step - loss: 0.4602 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 459us/step - loss: 0.4590 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 455us/step - loss: 0.4606 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 0s 460us/step - loss: 0.4598 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 0s 471us/step - loss: 0.4576 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 0s 454us/step - loss: 0.4584 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 0s 455us/step - loss: 0.4586 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 459us/step - loss: 0.4596 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 577us/step - loss: 0.4575 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 0s 505us/step - loss: 0.4586 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 716us/step - loss: 0.4583 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 675us/step - loss: 0.4585 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 470us/step - loss: 0.4584 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 573us/step - loss: 0.4565 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 779us/step - loss: 0.4595 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 0s 548us/step - loss: 0.4579 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 640us/step - loss: 0.4568 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 663us/step - loss: 0.4566 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 0s 546us/step - loss: 0.4569 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 0s 456us/step - loss: 0.4560 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 572us/step - loss: 0.4577 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 622us/step - loss: 0.4579 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 525us/step - loss: 0.4571 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 663us/step - loss: 0.4564 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 640us/step - loss: 0.4555 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 0s 531us/step - loss: 0.4572 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 711us/step - loss: 0.4572 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 454us/step - loss: 0.4563 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 0s 535us/step - loss: 0.4557 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 0s 496us/step - loss: 0.4565 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 649us/step - loss: 0.4577 - acc: 0.8838\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912/912 [==============================] - 1s 559us/step - loss: 0.4575 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 588us/step - loss: 0.4557 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 633us/step - loss: 0.4559 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 711us/step - loss: 0.4565 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 636us/step - loss: 0.4561 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 591us/step - loss: 0.4564 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 676us/step - loss: 0.4570 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 610us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 681us/step - loss: 0.4570 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 746us/step - loss: 0.4570 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 593us/step - loss: 0.4569 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 616us/step - loss: 0.4565 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 686us/step - loss: 0.4561 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 0s 524us/step - loss: 0.4560 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 633us/step - loss: 0.4558 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 612us/step - loss: 0.4575 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 747us/step - loss: 0.4565 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 632us/step - loss: 0.4563 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 743us/step - loss: 0.4552 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 599us/step - loss: 0.4574 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 650us/step - loss: 0.4568 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 582us/step - loss: 0.4572 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 613us/step - loss: 0.4567 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 556us/step - loss: 0.4559 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 618us/step - loss: 0.4565 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 585us/step - loss: 0.4565 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 694us/step - loss: 0.4559 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "AlgoCrossValIter - 4\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.5453 - acc: 0.8438Epoch 00001: val_loss improved from inf to 0.47789, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 922us/step - loss: 0.5503 - acc: 0.8436 - val_loss: 0.4779 - val_acc: 0.8816\n",
      "Epoch 2/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4664 - acc: 0.8828Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 1s 863us/step - loss: 0.4631 - acc: 0.8845 - val_loss: 0.5262 - val_acc: 0.8816\n",
      "Epoch 3/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4676 - acc: 0.8859Epoch 00003: val_loss improved from 0.47789 to 0.47437, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 896us/step - loss: 0.4708 - acc: 0.8845 - val_loss: 0.4744 - val_acc: 0.8816\n",
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4639 - acc: 0.8854Epoch 00004: val_loss improved from 0.47437 to 0.46874, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.4677 - acc: 0.8845 - val_loss: 0.4687 - val_acc: 0.8816\n",
      "Epoch 5/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4973 - acc: 0.8734Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 702us/step - loss: 0.4659 - acc: 0.8845 - val_loss: 0.4986 - val_acc: 0.8816\n",
      "Epoch 6/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4778 - acc: 0.8783Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 630us/step - loss: 0.4566 - acc: 0.8845 - val_loss: 0.4988 - val_acc: 0.8816\n",
      "Epoch 7/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4575 - acc: 0.8849Epoch 00007: val_loss improved from 0.46874 to 0.46579, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 663us/step - loss: 0.4595 - acc: 0.8845 - val_loss: 0.4658 - val_acc: 0.8816\n",
      "Epoch 8/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4371 - acc: 0.8914Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 629us/step - loss: 0.4561 - acc: 0.8845 - val_loss: 0.4661 - val_acc: 0.8816\n",
      "Epoch 9/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4541 - acc: 0.8854Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 1s 757us/step - loss: 0.4572 - acc: 0.8845 - val_loss: 0.4667 - val_acc: 0.8816\n",
      "Epoch 10/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4557 - acc: 0.8839Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 559us/step - loss: 0.4555 - acc: 0.8845 - val_loss: 0.4663 - val_acc: 0.8816\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4566 - acc: 0.8844Epoch 00001: val_loss improved from inf to 0.48009, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 839us/step - loss: 0.4498 - acc: 0.8874 - val_loss: 0.4801 - val_acc: 0.8728\n",
      "Epoch 2/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4422 - acc: 0.8884Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 0s 647us/step - loss: 0.4488 - acc: 0.8874 - val_loss: 0.4810 - val_acc: 0.8728\n",
      "Epoch 3/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4480 - acc: 0.8865Epoch 00003: val_loss improved from 0.48009 to 0.47942, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 797us/step - loss: 0.4493 - acc: 0.8874 - val_loss: 0.4794 - val_acc: 0.8728\n",
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4497 - acc: 0.8869Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 0s 688us/step - loss: 0.4473 - acc: 0.8874 - val_loss: 0.4841 - val_acc: 0.8728\n",
      "Epoch 5/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4518 - acc: 0.8869Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 534us/step - loss: 0.4495 - acc: 0.8874 - val_loss: 0.4804 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4441 - acc: 0.8884Epoch 00006: val_loss improved from 0.47942 to 0.47893, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 709us/step - loss: 0.4478 - acc: 0.8874 - val_loss: 0.4789 - val_acc: 0.8728\n",
      "Epoch 7/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4422 - acc: 0.8899Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 523us/step - loss: 0.4498 - acc: 0.8874 - val_loss: 0.4799 - val_acc: 0.8728\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "672/684 [============================>.] - ETA: 0s - loss: 0.4506 - acc: 0.8869Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 519us/step - loss: 0.4484 - acc: 0.8874 - val_loss: 0.4812 - val_acc: 0.8728\n",
      "Epoch 9/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4481 - acc: 0.8869Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 1s 779us/step - loss: 0.4487 - acc: 0.8874 - val_loss: 0.4817 - val_acc: 0.8728\n",
      "Epoch 10/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4455 - acc: 0.8891Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 669us/step - loss: 0.4480 - acc: 0.8874 - val_loss: 0.4815 - val_acc: 0.8728\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4815 - acc: 0.8750Epoch 00001: val_loss improved from inf to 0.39238, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 860us/step - loss: 0.4787 - acc: 0.8757 - val_loss: 0.3924 - val_acc: 0.9079\n",
      "Epoch 2/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4786 - acc: 0.8766Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 0s 672us/step - loss: 0.4764 - acc: 0.8757 - val_loss: 0.3969 - val_acc: 0.9079\n",
      "Epoch 3/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4728 - acc: 0.8780Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 1s 892us/step - loss: 0.4772 - acc: 0.8757 - val_loss: 0.4015 - val_acc: 0.9079\n",
      "Epoch 4/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4769 - acc: 0.8750Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 1s 799us/step - loss: 0.4769 - acc: 0.8757 - val_loss: 0.4104 - val_acc: 0.9079\n",
      "Epoch 5/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4698 - acc: 0.8765Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 691us/step - loss: 0.4729 - acc: 0.8757 - val_loss: 0.4027 - val_acc: 0.9079\n",
      "Epoch 6/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4833 - acc: 0.8735Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 1s 740us/step - loss: 0.4769 - acc: 0.8757 - val_loss: 0.4059 - val_acc: 0.9079\n",
      "Epoch 7/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4932 - acc: 0.8688Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 608us/step - loss: 0.4743 - acc: 0.8757 - val_loss: 0.4092 - val_acc: 0.9079\n",
      "Epoch 8/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4725 - acc: 0.8802Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 696us/step - loss: 0.4799 - acc: 0.8757 - val_loss: 0.4164 - val_acc: 0.9079\n",
      "Epoch 9/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4744 - acc: 0.8766Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 1s 837us/step - loss: 0.4756 - acc: 0.8757 - val_loss: 0.4192 - val_acc: 0.9079\n",
      "Epoch 10/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4911 - acc: 0.8717Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 647us/step - loss: 0.4767 - acc: 0.8757 - val_loss: 0.4205 - val_acc: 0.9079\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4599 - acc: 0.8869Epoch 00001: val_loss improved from inf to 0.48029, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 697us/step - loss: 0.4575 - acc: 0.8874 - val_loss: 0.4803 - val_acc: 0.8728\n",
      "Epoch 2/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4537 - acc: 0.8884Epoch 00002: val_loss improved from 0.48029 to 0.47945, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.4570 - acc: 0.8874 - val_loss: 0.4795 - val_acc: 0.8728\n",
      "Epoch 3/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4617 - acc: 0.8854Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 0s 653us/step - loss: 0.4560 - acc: 0.8874 - val_loss: 0.4816 - val_acc: 0.8728\n",
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4496 - acc: 0.8884Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 0s 697us/step - loss: 0.4560 - acc: 0.8874 - val_loss: 0.4795 - val_acc: 0.8728\n",
      "Epoch 5/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4454 - acc: 0.8865Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 712us/step - loss: 0.4562 - acc: 0.8874 - val_loss: 0.4816 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4665 - acc: 0.8849Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 1s 832us/step - loss: 0.4551 - acc: 0.8874 - val_loss: 0.4863 - val_acc: 0.8728\n",
      "Epoch 7/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4535 - acc: 0.8884Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 537us/step - loss: 0.4550 - acc: 0.8874 - val_loss: 0.4830 - val_acc: 0.8728\n",
      "Epoch 8/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4649 - acc: 0.8849Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 642us/step - loss: 0.4531 - acc: 0.8874 - val_loss: 0.4876 - val_acc: 0.8728\n",
      "Epoch 9/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4572 - acc: 0.8869Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 727us/step - loss: 0.4548 - acc: 0.8874 - val_loss: 0.4804 - val_acc: 0.8728\n",
      "Epoch 10/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4540 - acc: 0.8898Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 578us/step - loss: 0.4540 - acc: 0.8874 - val_loss: 0.4815 - val_acc: 0.8728\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 561us/step - loss: 0.4626 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 0s 516us/step - loss: 0.4596 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 0s 520us/step - loss: 0.4621 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 510us/step - loss: 0.4611 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 528us/step - loss: 0.4592 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 0s 547us/step - loss: 0.4588 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 649us/step - loss: 0.4599 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 624us/step - loss: 0.4592 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 0s 527us/step - loss: 0.4592 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 616us/step - loss: 0.4595 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 0s 514us/step - loss: 0.4589 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 566us/step - loss: 0.4571 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 0s 526us/step - loss: 0.4567 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 551us/step - loss: 0.4581 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 488us/step - loss: 0.4571 - acc: 0.8838\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912/912 [==============================] - 0s 538us/step - loss: 0.4576 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 578us/step - loss: 0.4567 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 567us/step - loss: 0.4584 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 686us/step - loss: 0.4562 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 692us/step - loss: 0.4572 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 628us/step - loss: 0.4564 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 561us/step - loss: 0.4578 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 699us/step - loss: 0.4567 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 560us/step - loss: 0.4567 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 707us/step - loss: 0.4554 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 687us/step - loss: 0.4582 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 623us/step - loss: 0.4568 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 591us/step - loss: 0.4564 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 645us/step - loss: 0.4564 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 775us/step - loss: 0.4564 - acc: 0.8838 0s - loss: 0.4628 - acc: 0.8 - ETA: 0s - loss: 0.4816 - acc: 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 639us/step - loss: 0.4568 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 578us/step - loss: 0.4570 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 624us/step - loss: 0.4558 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 673us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 785us/step - loss: 0.4575 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 564us/step - loss: 0.4571 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 0s 541us/step - loss: 0.4565 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 639us/step - loss: 0.4566 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 825us/step - loss: 0.4554 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 691us/step - loss: 0.4561 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 0s 547us/step - loss: 0.4564 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 669us/step - loss: 0.4570 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 611us/step - loss: 0.4566 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 624us/step - loss: 0.4550 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 590us/step - loss: 0.4577 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 614us/step - loss: 0.4577 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 658us/step - loss: 0.4559 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 738us/step - loss: 0.4567 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 624us/step - loss: 0.4562 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 581us/step - loss: 0.4568 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 716us/step - loss: 0.4575 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 634us/step - loss: 0.4568 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 598us/step - loss: 0.4563 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 528us/step - loss: 0.4560 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 633us/step - loss: 0.4563 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 562us/step - loss: 0.4570 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 666us/step - loss: 0.4568 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 0s 538us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 597us/step - loss: 0.4588 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 607us/step - loss: 0.4566 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "AlgoCrossValIter - 5\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.6116 - acc: 0.8355Epoch 00001: val_loss improved from inf to 0.47059, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 931us/step - loss: 0.5889 - acc: 0.8436 - val_loss: 0.4706 - val_acc: 0.8816\n",
      "Epoch 2/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.5214 - acc: 0.8344Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 1s 899us/step - loss: 0.5012 - acc: 0.8421 - val_loss: 0.5130 - val_acc: 0.8816\n",
      "Epoch 3/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4667 - acc: 0.8854Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 1s 763us/step - loss: 0.4672 - acc: 0.8845 - val_loss: 0.5075 - val_acc: 0.8816\n",
      "Epoch 4/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4692 - acc: 0.8859Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 1s 840us/step - loss: 0.4761 - acc: 0.8845 - val_loss: 0.5879 - val_acc: 0.8816\n",
      "Epoch 5/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4780 - acc: 0.8854Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 538us/step - loss: 0.4786 - acc: 0.8845 - val_loss: 0.4914 - val_acc: 0.8816\n",
      "Epoch 6/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4546 - acc: 0.8869Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 527us/step - loss: 0.4584 - acc: 0.8845 - val_loss: 0.6602 - val_acc: 0.8816\n",
      "Epoch 7/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4544 - acc: 0.8882Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 551us/step - loss: 0.4638 - acc: 0.8845 - val_loss: 0.5351 - val_acc: 0.8816\n",
      "Epoch 8/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4679 - acc: 0.8824Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 506us/step - loss: 0.4616 - acc: 0.8845 - val_loss: 0.4859 - val_acc: 0.8816\n",
      "Epoch 9/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4641 - acc: 0.8839Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 513us/step - loss: 0.4617 - acc: 0.8845 - val_loss: 0.4786 - val_acc: 0.8816\n",
      "Epoch 10/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4529 - acc: 0.8859Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 657us/step - loss: 0.4601 - acc: 0.8845 - val_loss: 0.4759 - val_acc: 0.8816\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4548 - acc: 0.8875Epoch 00001: val_loss improved from inf to 0.47885, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 931us/step - loss: 0.4535 - acc: 0.8874 - val_loss: 0.4788 - val_acc: 0.8728\n",
      "Epoch 2/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4555 - acc: 0.8865Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 1s 810us/step - loss: 0.4482 - acc: 0.8874 - val_loss: 0.4857 - val_acc: 0.8728\n",
      "Epoch 3/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4409 - acc: 0.8891Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 0s 690us/step - loss: 0.4517 - acc: 0.8874 - val_loss: 0.4819 - val_acc: 0.8728\n",
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4522 - acc: 0.8869Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 1s 758us/step - loss: 0.4499 - acc: 0.8874 - val_loss: 0.4852 - val_acc: 0.8728\n",
      "Epoch 5/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4414 - acc: 0.8889Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 591us/step - loss: 0.4495 - acc: 0.8874 - val_loss: 0.4814 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4547 - acc: 0.8837Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 1s 734us/step - loss: 0.4495 - acc: 0.8874 - val_loss: 0.4848 - val_acc: 0.8728\n",
      "Epoch 7/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4511 - acc: 0.8869Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 1s 803us/step - loss: 0.4489 - acc: 0.8874 - val_loss: 0.4814 - val_acc: 0.8728\n",
      "Epoch 8/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4437 - acc: 0.8891Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 569us/step - loss: 0.4473 - acc: 0.8874 - val_loss: 0.4884 - val_acc: 0.8728\n",
      "Epoch 9/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4407 - acc: 0.8891Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 1s 812us/step - loss: 0.4497 - acc: 0.8874 - val_loss: 0.4827 - val_acc: 0.8728\n",
      "Epoch 10/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4216 - acc: 0.8964Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 1s 792us/step - loss: 0.4485 - acc: 0.8874 - val_loss: 0.4862 - val_acc: 0.8728\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4792 - acc: 0.8766Epoch 00001: val_loss improved from inf to 0.39472, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 733us/step - loss: 0.4795 - acc: 0.8757 - val_loss: 0.3947 - val_acc: 0.9079\n",
      "Epoch 2/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4830 - acc: 0.8734Epoch 00002: val_loss improved from 0.39472 to 0.39002, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 750us/step - loss: 0.4737 - acc: 0.8757 - val_loss: 0.3900 - val_acc: 0.9079\n",
      "Epoch 3/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4738 - acc: 0.8766Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 1s 809us/step - loss: 0.4778 - acc: 0.8757 - val_loss: 0.3988 - val_acc: 0.9079\n",
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4732 - acc: 0.8765Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 0s 631us/step - loss: 0.4762 - acc: 0.8757 - val_loss: 0.4071 - val_acc: 0.9079\n",
      "Epoch 5/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4811 - acc: 0.8734Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 625us/step - loss: 0.4771 - acc: 0.8757 - val_loss: 0.4031 - val_acc: 0.9079\n",
      "Epoch 6/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4610 - acc: 0.8799Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 677us/step - loss: 0.4763 - acc: 0.8757 - val_loss: 0.4274 - val_acc: 0.9079\n",
      "Epoch 7/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4819 - acc: 0.8734Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 1s 796us/step - loss: 0.4774 - acc: 0.8757 - val_loss: 0.4123 - val_acc: 0.9079\n",
      "Epoch 8/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4814 - acc: 0.8734Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 648us/step - loss: 0.4758 - acc: 0.8757 - val_loss: 0.4161 - val_acc: 0.9079\n",
      "Epoch 9/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4730 - acc: 0.8785Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 622us/step - loss: 0.4760 - acc: 0.8757 - val_loss: 0.4196 - val_acc: 0.9079\n",
      "Epoch 10/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4800 - acc: 0.8750Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 675us/step - loss: 0.4773 - acc: 0.8757 - val_loss: 0.4209 - val_acc: 0.9079\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4467 - acc: 0.8898Epoch 00001: val_loss improved from inf to 0.47931, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 619us/step - loss: 0.4579 - acc: 0.8874 - val_loss: 0.4793 - val_acc: 0.8728\n",
      "Epoch 2/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4707 - acc: 0.8828Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 1s 735us/step - loss: 0.4572 - acc: 0.8874 - val_loss: 0.4858 - val_acc: 0.8728\n",
      "Epoch 3/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4827 - acc: 0.8819Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 0s 585us/step - loss: 0.4580 - acc: 0.8874 - val_loss: 0.4826 - val_acc: 0.8728\n",
      "Epoch 4/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4426 - acc: 0.8931Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 1s 798us/step - loss: 0.4535 - acc: 0.8874 - val_loss: 0.4815 - val_acc: 0.8728\n",
      "Epoch 5/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4613 - acc: 0.8854Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 581us/step - loss: 0.4554 - acc: 0.8874 - val_loss: 0.4850 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4561 - acc: 0.8882Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 717us/step - loss: 0.4551 - acc: 0.8874 - val_loss: 0.4793 - val_acc: 0.8728\n",
      "Epoch 7/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4556 - acc: 0.8869Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 706us/step - loss: 0.4555 - acc: 0.8874 - val_loss: 0.4839 - val_acc: 0.8728\n",
      "Epoch 8/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4448 - acc: 0.8849Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 604us/step - loss: 0.4531 - acc: 0.8874 - val_loss: 0.4835 - val_acc: 0.8728\n",
      "Epoch 9/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4496 - acc: 0.8884Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 1s 861us/step - loss: 0.4538 - acc: 0.8874 - val_loss: 0.4796 - val_acc: 0.8728\n",
      "Epoch 10/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4761 - acc: 0.8799Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 1s 753us/step - loss: 0.4534 - acc: 0.8874 - val_loss: 0.4874 - val_acc: 0.8728\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 643us/step - loss: 0.4594 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 663us/step - loss: 0.4610 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 720us/step - loss: 0.4593 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 585us/step - loss: 0.4590 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 651us/step - loss: 0.4605 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 549us/step - loss: 0.4606 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 583us/step - loss: 0.4567 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 649us/step - loss: 0.4578 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 0s 543us/step - loss: 0.4565 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 542us/step - loss: 0.4571 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 602us/step - loss: 0.4567 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 0s 514us/step - loss: 0.4577 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 640us/step - loss: 0.4590 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 620us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 519us/step - loss: 0.4579 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 644us/step - loss: 0.4580 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 557us/step - loss: 0.4564 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 0s 535us/step - loss: 0.4567 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 669us/step - loss: 0.4578 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 630us/step - loss: 0.4574 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 622us/step - loss: 0.4577 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 578us/step - loss: 0.4552 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 0s 481us/step - loss: 0.4564 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 689us/step - loss: 0.4580 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 524us/step - loss: 0.4583 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 573us/step - loss: 0.4577 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 576us/step - loss: 0.4589 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 571us/step - loss: 0.4566 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 622us/step - loss: 0.4578 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 637us/step - loss: 0.4566 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 594us/step - loss: 0.4579 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 560us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 557us/step - loss: 0.4563 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 533us/step - loss: 0.4572 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 676us/step - loss: 0.4563 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 708us/step - loss: 0.4574 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 661us/step - loss: 0.4575 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 615us/step - loss: 0.4571 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 685us/step - loss: 0.4558 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 524us/step - loss: 0.4575 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 583us/step - loss: 0.4549 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 691us/step - loss: 0.4586 - acc: 0.8838 0s - loss: 0.4511 - acc: 0.8\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 777us/step - loss: 0.4556 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 781us/step - loss: 0.4574 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 617us/step - loss: 0.4574 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 726us/step - loss: 0.4567 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 795us/step - loss: 0.4572 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 722us/step - loss: 0.4568 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 0s 505us/step - loss: 0.4561 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 543us/step - loss: 0.4576 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 662us/step - loss: 0.4560 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 0s 468us/step - loss: 0.4572 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 0s 451us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 451us/step - loss: 0.4544 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 459us/step - loss: 0.4567 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 701us/step - loss: 0.4560 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 0s 477us/step - loss: 0.4566 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 725us/step - loss: 0.4568 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 643us/step - loss: 0.4568 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 540us/step - loss: 0.4576 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "AlgoCrossValIter - 6\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.5416 - acc: 0.8351Epoch 00001: val_loss improved from inf to 0.49018, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 907us/step - loss: 0.5294 - acc: 0.8436 - val_loss: 0.4902 - val_acc: 0.8816\n",
      "Epoch 2/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4707 - acc: 0.8839Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 1s 733us/step - loss: 0.4744 - acc: 0.8845 - val_loss: 0.4982 - val_acc: 0.8816\n",
      "Epoch 3/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4838 - acc: 0.8824Epoch 00003: val_loss improved from 0.49018 to 0.48915, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 694us/step - loss: 0.4769 - acc: 0.8845 - val_loss: 0.4892 - val_acc: 0.8816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4607 - acc: 0.8854Epoch 00004: val_loss improved from 0.48915 to 0.47871, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.4616 - acc: 0.8845 - val_loss: 0.4787 - val_acc: 0.8816\n",
      "Epoch 5/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4617 - acc: 0.8844Epoch 00005: val_loss improved from 0.47871 to 0.46559, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.4665 - acc: 0.8845 - val_loss: 0.4656 - val_acc: 0.8816\n",
      "Epoch 6/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4574 - acc: 0.8865Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 634us/step - loss: 0.4611 - acc: 0.8845 - val_loss: 0.4711 - val_acc: 0.8816\n",
      "Epoch 7/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4542 - acc: 0.8859Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 730us/step - loss: 0.4556 - acc: 0.8845 - val_loss: 0.4697 - val_acc: 0.8816\n",
      "Epoch 8/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4342 - acc: 0.8906Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 706us/step - loss: 0.4580 - acc: 0.8845 - val_loss: 0.4715 - val_acc: 0.8816\n",
      "Epoch 9/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4638 - acc: 0.8824Epoch 00009: val_loss improved from 0.46559 to 0.46556, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 810us/step - loss: 0.4584 - acc: 0.8845 - val_loss: 0.4656 - val_acc: 0.8816\n",
      "Epoch 10/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4764 - acc: 0.8766Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 1s 791us/step - loss: 0.4525 - acc: 0.8845 - val_loss: 0.4734 - val_acc: 0.8816\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4498 - acc: 0.8869Epoch 00001: val_loss improved from inf to 0.47749, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 701us/step - loss: 0.4501 - acc: 0.8874 - val_loss: 0.4775 - val_acc: 0.8728\n",
      "Epoch 2/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4723 - acc: 0.8816Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 0s 696us/step - loss: 0.4510 - acc: 0.8874 - val_loss: 0.4804 - val_acc: 0.8728\n",
      "Epoch 3/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4505 - acc: 0.8865Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 1s 788us/step - loss: 0.4496 - acc: 0.8874 - val_loss: 0.4790 - val_acc: 0.8728\n",
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4478 - acc: 0.8869Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 0s 680us/step - loss: 0.4479 - acc: 0.8874 - val_loss: 0.4832 - val_acc: 0.8728\n",
      "Epoch 5/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4514 - acc: 0.8869Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 661us/step - loss: 0.4490 - acc: 0.8874 - val_loss: 0.4848 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4517 - acc: 0.8859Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 705us/step - loss: 0.4492 - acc: 0.8874 - val_loss: 0.4806 - val_acc: 0.8728\n",
      "Epoch 7/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4602 - acc: 0.8844Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 1s 843us/step - loss: 0.4487 - acc: 0.8874 - val_loss: 0.4829 - val_acc: 0.8728\n",
      "Epoch 8/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4663 - acc: 0.8819Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 643us/step - loss: 0.4475 - acc: 0.8874 - val_loss: 0.4920 - val_acc: 0.8728\n",
      "Epoch 9/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4454 - acc: 0.8882Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 604us/step - loss: 0.4489 - acc: 0.8874 - val_loss: 0.4804 - val_acc: 0.8728\n",
      "Epoch 10/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4352 - acc: 0.8914Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 705us/step - loss: 0.4481 - acc: 0.8874 - val_loss: 0.4821 - val_acc: 0.8728\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4604 - acc: 0.8849Epoch 00001: val_loss improved from inf to 0.40297, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 602us/step - loss: 0.4789 - acc: 0.8757 - val_loss: 0.4030 - val_acc: 0.9079\n",
      "Epoch 2/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4670 - acc: 0.8795Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 0s 674us/step - loss: 0.4772 - acc: 0.8757 - val_loss: 0.4041 - val_acc: 0.9079\n",
      "Epoch 3/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4825 - acc: 0.8735Epoch 00003: val_loss improved from 0.40297 to 0.39515, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 983us/step - loss: 0.4764 - acc: 0.8757 - val_loss: 0.3951 - val_acc: 0.9079\n",
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4757 - acc: 0.8765Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 1s 892us/step - loss: 0.4763 - acc: 0.8757 - val_loss: 0.4070 - val_acc: 0.9079\n",
      "Epoch 5/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4723 - acc: 0.8780Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 527us/step - loss: 0.4757 - acc: 0.8757 - val_loss: 0.4358 - val_acc: 0.9079\n",
      "Epoch 6/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4789 - acc: 0.8750Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 515us/step - loss: 0.4781 - acc: 0.8757 - val_loss: 0.4087 - val_acc: 0.9079\n",
      "Epoch 7/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4479 - acc: 0.8854Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 603us/step - loss: 0.4773 - acc: 0.8757 - val_loss: 0.4166 - val_acc: 0.9079\n",
      "Epoch 8/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4877 - acc: 0.8719Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 611us/step - loss: 0.4764 - acc: 0.8757 - val_loss: 0.4154 - val_acc: 0.9079\n",
      "Epoch 9/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4774 - acc: 0.8767Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 630us/step - loss: 0.4774 - acc: 0.8757 - val_loss: 0.4188 - val_acc: 0.9079\n",
      "Epoch 10/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4836 - acc: 0.8734Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 598us/step - loss: 0.4765 - acc: 0.8757 - val_loss: 0.4227 - val_acc: 0.9079\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4607 - acc: 0.8869Epoch 00001: val_loss improved from inf to 0.48025, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 721us/step - loss: 0.4583 - acc: 0.8874 - val_loss: 0.4803 - val_acc: 0.8728\n",
      "Epoch 2/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4714 - acc: 0.8844Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 0s 557us/step - loss: 0.4588 - acc: 0.8874 - val_loss: 0.4859 - val_acc: 0.8728\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4503 - acc: 0.8891Epoch 00003: val_loss improved from 0.48025 to 0.47968, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 805us/step - loss: 0.4582 - acc: 0.8874 - val_loss: 0.4797 - val_acc: 0.8728\n",
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4549 - acc: 0.8869Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 0s 686us/step - loss: 0.4546 - acc: 0.8874 - val_loss: 0.4895 - val_acc: 0.8728\n",
      "Epoch 5/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4616 - acc: 0.8854Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 1s 785us/step - loss: 0.4554 - acc: 0.8874 - val_loss: 0.4912 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4628 - acc: 0.8854Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 1s 827us/step - loss: 0.4568 - acc: 0.8874 - val_loss: 0.4867 - val_acc: 0.8728\n",
      "Epoch 7/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4567 - acc: 0.8882Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 1s 742us/step - loss: 0.4560 - acc: 0.8874 - val_loss: 0.4808 - val_acc: 0.8728\n",
      "Epoch 8/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4677 - acc: 0.8832Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 1s 793us/step - loss: 0.4545 - acc: 0.8874 - val_loss: 0.4844 - val_acc: 0.8728\n",
      "Epoch 9/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4774 - acc: 0.8799Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 573us/step - loss: 0.4574 - acc: 0.8874 - val_loss: 0.4838 - val_acc: 0.8728\n",
      "Epoch 10/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4599 - acc: 0.8872Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 611us/step - loss: 0.4534 - acc: 0.8874 - val_loss: 0.4803 - val_acc: 0.8728\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 625us/step - loss: 0.4607 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 606us/step - loss: 0.4623 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 0s 525us/step - loss: 0.4608 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 658us/step - loss: 0.4613 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 661us/step - loss: 0.4591 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 0s 534us/step - loss: 0.4617 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 612us/step - loss: 0.4610 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 677us/step - loss: 0.4603 - acc: 0.8838 0s - loss: 0.4429 - acc: 0\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 581us/step - loss: 0.4592 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 552us/step - loss: 0.4591 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 695us/step - loss: 0.4572 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 0s 517us/step - loss: 0.4592 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 611us/step - loss: 0.4583 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 541us/step - loss: 0.4586 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 576us/step - loss: 0.4572 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 549us/step - loss: 0.4580 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 0s 522us/step - loss: 0.4584 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 572us/step - loss: 0.4589 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 580us/step - loss: 0.4582 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 464us/step - loss: 0.4582 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 586us/step - loss: 0.4572 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 687us/step - loss: 0.4581 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 621us/step - loss: 0.4575 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 516us/step - loss: 0.4580 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 626us/step - loss: 0.4585 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 594us/step - loss: 0.4572 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 570us/step - loss: 0.4571 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 782us/step - loss: 0.4562 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 0s 485us/step - loss: 0.4578 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 629us/step - loss: 0.4589 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 608us/step - loss: 0.4570 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 555us/step - loss: 0.4570 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 0s 525us/step - loss: 0.4582 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 539us/step - loss: 0.4578 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 599us/step - loss: 0.4545 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 0s 527us/step - loss: 0.4589 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 557us/step - loss: 0.4564 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 558us/step - loss: 0.4560 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 580us/step - loss: 0.4589 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 528us/step - loss: 0.4566 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 582us/step - loss: 0.4570 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 607us/step - loss: 0.4583 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 604us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 534us/step - loss: 0.4558 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 823us/step - loss: 0.4554 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 0s 484us/step - loss: 0.4582 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 568us/step - loss: 0.4571 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 0s 509us/step - loss: 0.4569 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 0s 458us/step - loss: 0.4558 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 472us/step - loss: 0.4574 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912/912 [==============================] - 1s 759us/step - loss: 0.4574 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 627us/step - loss: 0.4576 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 650us/step - loss: 0.4583 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 700us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 804us/step - loss: 0.4566 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 666us/step - loss: 0.4563 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 731us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 0s 534us/step - loss: 0.4562 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 0s 547us/step - loss: 0.4582 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 538us/step - loss: 0.4558 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "AlgoCrossValIter - 7\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.5854 - acc: 0.8273Epoch 00001: val_loss improved from inf to 0.47636, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 868us/step - loss: 0.5508 - acc: 0.8406 - val_loss: 0.4764 - val_acc: 0.8816\n",
      "Epoch 2/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4837 - acc: 0.8849Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 1s 857us/step - loss: 0.4869 - acc: 0.8845 - val_loss: 0.4814 - val_acc: 0.8816\n",
      "Epoch 3/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4841 - acc: 0.8837Epoch 00003: val_loss improved from 0.47636 to 0.47350, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 784us/step - loss: 0.4753 - acc: 0.8845 - val_loss: 0.4735 - val_acc: 0.8816\n",
      "Epoch 4/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4535 - acc: 0.8889Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 0s 610us/step - loss: 0.4695 - acc: 0.8845 - val_loss: 0.4740 - val_acc: 0.8816\n",
      "Epoch 5/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4556 - acc: 0.8837Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 1s 738us/step - loss: 0.4597 - acc: 0.8845 - val_loss: 0.4993 - val_acc: 0.8816\n",
      "Epoch 6/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4592 - acc: 0.8854Epoch 00006: val_loss improved from 0.47350 to 0.46919, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 851us/step - loss: 0.4626 - acc: 0.8845 - val_loss: 0.4692 - val_acc: 0.8816\n",
      "Epoch 7/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4582 - acc: 0.8828Epoch 00007: val_loss improved from 0.46919 to 0.46685, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 925us/step - loss: 0.4611 - acc: 0.8845 - val_loss: 0.4669 - val_acc: 0.8816\n",
      "Epoch 8/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4484 - acc: 0.8882Epoch 00008: val_loss improved from 0.46685 to 0.46672, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 829us/step - loss: 0.4572 - acc: 0.8845 - val_loss: 0.4667 - val_acc: 0.8816\n",
      "Epoch 9/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4482 - acc: 0.8869Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 1s 797us/step - loss: 0.4567 - acc: 0.8845 - val_loss: 0.4680 - val_acc: 0.8816\n",
      "Epoch 10/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4528 - acc: 0.8849Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 1s 929us/step - loss: 0.4563 - acc: 0.8845 - val_loss: 0.4677 - val_acc: 0.8816\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4418 - acc: 0.8882Epoch 00001: val_loss improved from inf to 0.48083, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 730us/step - loss: 0.4501 - acc: 0.8874 - val_loss: 0.4808 - val_acc: 0.8728\n",
      "Epoch 2/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4642 - acc: 0.8816Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 0s 619us/step - loss: 0.4478 - acc: 0.8874 - val_loss: 0.4977 - val_acc: 0.8728\n",
      "Epoch 3/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4518 - acc: 0.8884Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 0s 546us/step - loss: 0.4534 - acc: 0.8874 - val_loss: 0.4828 - val_acc: 0.8728\n",
      "Epoch 4/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4614 - acc: 0.8832Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 0s 609us/step - loss: 0.4465 - acc: 0.8874 - val_loss: 0.4894 - val_acc: 0.8728\n",
      "Epoch 5/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4403 - acc: 0.8931Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 636us/step - loss: 0.4494 - acc: 0.8874 - val_loss: 0.4809 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4507 - acc: 0.8882Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 1s 856us/step - loss: 0.4485 - acc: 0.8874 - val_loss: 0.4816 - val_acc: 0.8728\n",
      "Epoch 7/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4556 - acc: 0.8854Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 691us/step - loss: 0.4497 - acc: 0.8874 - val_loss: 0.4840 - val_acc: 0.8728\n",
      "Epoch 8/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4541 - acc: 0.8844Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 1s 761us/step - loss: 0.4478 - acc: 0.8874 - val_loss: 0.4916 - val_acc: 0.8728\n",
      "Epoch 9/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4563 - acc: 0.8854Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 539us/step - loss: 0.4500 - acc: 0.8874 - val_loss: 0.4924 - val_acc: 0.8728\n",
      "Epoch 10/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4454 - acc: 0.8884Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 563us/step - loss: 0.4494 - acc: 0.8874 - val_loss: 0.4823 - val_acc: 0.8728\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4689 - acc: 0.8781Epoch 00001: val_loss improved from inf to 0.39416, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 716us/step - loss: 0.4774 - acc: 0.8757 - val_loss: 0.3942 - val_acc: 0.9079\n",
      "Epoch 2/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4786 - acc: 0.8766Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 0s 682us/step - loss: 0.4763 - acc: 0.8757 - val_loss: 0.4106 - val_acc: 0.9079\n",
      "Epoch 3/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4769 - acc: 0.8765Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 1s 762us/step - loss: 0.4796 - acc: 0.8757 - val_loss: 0.4044 - val_acc: 0.9079\n",
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4823 - acc: 0.8735Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 0s 522us/step - loss: 0.4761 - acc: 0.8757 - val_loss: 0.3976 - val_acc: 0.9079\n",
      "Epoch 5/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4784 - acc: 0.8750Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 522us/step - loss: 0.4756 - acc: 0.8757 - val_loss: 0.4004 - val_acc: 0.9079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4605 - acc: 0.8799Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 595us/step - loss: 0.4759 - acc: 0.8757 - val_loss: 0.4053 - val_acc: 0.9079\n",
      "Epoch 7/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4790 - acc: 0.8750Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 1s 759us/step - loss: 0.4762 - acc: 0.8757 - val_loss: 0.4086 - val_acc: 0.9079\n",
      "Epoch 8/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4844 - acc: 0.8734Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 590us/step - loss: 0.4763 - acc: 0.8757 - val_loss: 0.4134 - val_acc: 0.9079\n",
      "Epoch 9/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4652 - acc: 0.8766Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 638us/step - loss: 0.4755 - acc: 0.8757 - val_loss: 0.4314 - val_acc: 0.9079\n",
      "Epoch 10/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4743 - acc: 0.8750Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 1s 763us/step - loss: 0.4763 - acc: 0.8757 - val_loss: 0.4193 - val_acc: 0.9079\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4524 - acc: 0.8884Epoch 00001: val_loss improved from inf to 0.47941, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 818us/step - loss: 0.4566 - acc: 0.8874 - val_loss: 0.4794 - val_acc: 0.8728\n",
      "Epoch 2/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4529 - acc: 0.8865Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 1s 879us/step - loss: 0.4572 - acc: 0.8874 - val_loss: 0.4875 - val_acc: 0.8728\n",
      "Epoch 3/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4636 - acc: 0.8854Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 1s 760us/step - loss: 0.4572 - acc: 0.8874 - val_loss: 0.4911 - val_acc: 0.8728\n",
      "Epoch 4/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4757 - acc: 0.8812Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 1s 747us/step - loss: 0.4582 - acc: 0.8874 - val_loss: 0.4899 - val_acc: 0.8728\n",
      "Epoch 5/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4526 - acc: 0.8884Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 1s 924us/step - loss: 0.4561 - acc: 0.8874 - val_loss: 0.4795 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4684 - acc: 0.8844Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 1s 786us/step - loss: 0.4564 - acc: 0.8874 - val_loss: 0.4822 - val_acc: 0.8728\n",
      "Epoch 7/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4488 - acc: 0.8906Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 717us/step - loss: 0.4539 - acc: 0.8874 - val_loss: 0.4825 - val_acc: 0.8728\n",
      "Epoch 8/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4595 - acc: 0.8859Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 664us/step - loss: 0.4537 - acc: 0.8874 - val_loss: 0.4845 - val_acc: 0.8728\n",
      "Epoch 9/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4564 - acc: 0.8869Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 557us/step - loss: 0.4540 - acc: 0.8874 - val_loss: 0.4806 - val_acc: 0.8728\n",
      "Epoch 10/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4609 - acc: 0.8854Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 1s 901us/step - loss: 0.4549 - acc: 0.8874 - val_loss: 0.4863 - val_acc: 0.8728\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 0s 522us/step - loss: 0.4618 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 613us/step - loss: 0.4606 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 751us/step - loss: 0.4613 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 539us/step - loss: 0.4591 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 470us/step - loss: 0.4607 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 0s 484us/step - loss: 0.4607 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 572us/step - loss: 0.4607 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 574us/step - loss: 0.4594 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 864us/step - loss: 0.4576 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 526us/step - loss: 0.4584 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 657us/step - loss: 0.4612 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 566us/step - loss: 0.4588 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 576us/step - loss: 0.4590 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 494us/step - loss: 0.4583 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 487us/step - loss: 0.4569 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 0s 491us/step - loss: 0.4588 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 0s 532us/step - loss: 0.4564 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 578us/step - loss: 0.4581 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 0s 494us/step - loss: 0.4565 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 535us/step - loss: 0.4584 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 775us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 623us/step - loss: 0.4568 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 704us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 698us/step - loss: 0.4575 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 691us/step - loss: 0.4557 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 742us/step - loss: 0.4568 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 702us/step - loss: 0.4576 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 743us/step - loss: 0.4568 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 0s 479us/step - loss: 0.4588 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 503us/step - loss: 0.4551 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 0s 466us/step - loss: 0.4587 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 0s 527us/step - loss: 0.4563 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 601us/step - loss: 0.4568 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 544us/step - loss: 0.4580 - acc: 0.8838\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912/912 [==============================] - 1s 638us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 561us/step - loss: 0.4574 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 631us/step - loss: 0.4577 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 727us/step - loss: 0.4569 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 715us/step - loss: 0.4561 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 749us/step - loss: 0.4579 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 744us/step - loss: 0.4580 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 839us/step - loss: 0.4565 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 692us/step - loss: 0.4571 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 713us/step - loss: 0.4562 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 906us/step - loss: 0.4566 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 662us/step - loss: 0.4555 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 616us/step - loss: 0.4571 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 733us/step - loss: 0.4565 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 585us/step - loss: 0.4576 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 678us/step - loss: 0.4585 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 761us/step - loss: 0.4555 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 723us/step - loss: 0.4570 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 980us/step - loss: 0.4576 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 797us/step - loss: 0.4556 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 737us/step - loss: 0.4572 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 737us/step - loss: 0.4567 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 586us/step - loss: 0.4580 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 613us/step - loss: 0.4569 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 562us/step - loss: 0.4571 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 554us/step - loss: 0.4568 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "AlgoCrossValIter - 8\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.5408 - acc: 0.8453Epoch 00001: val_loss improved from inf to 0.52037, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 987us/step - loss: 0.5319 - acc: 0.8480 - val_loss: 0.5204 - val_acc: 0.8816\n",
      "Epoch 2/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4688 - acc: 0.8875Epoch 00002: val_loss improved from 0.52037 to 0.51939, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.4780 - acc: 0.8845 - val_loss: 0.5194 - val_acc: 0.8816\n",
      "Epoch 3/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4662 - acc: 0.8869Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 0s 649us/step - loss: 0.4713 - acc: 0.8845 - val_loss: 0.5291 - val_acc: 0.8816\n",
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4695 - acc: 0.8854Epoch 00004: val_loss improved from 0.51939 to 0.49743, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 661us/step - loss: 0.4698 - acc: 0.8845 - val_loss: 0.4974 - val_acc: 0.8816\n",
      "Epoch 5/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4729 - acc: 0.8812Epoch 00005: val_loss improved from 0.49743 to 0.49273, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.4688 - acc: 0.8845 - val_loss: 0.4927 - val_acc: 0.8816\n",
      "Epoch 6/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4636 - acc: 0.8828Epoch 00006: val_loss improved from 0.49273 to 0.46392, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 964us/step - loss: 0.4608 - acc: 0.8845 - val_loss: 0.4639 - val_acc: 0.8816\n",
      "Epoch 7/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4677 - acc: 0.8839Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 556us/step - loss: 0.4649 - acc: 0.8845 - val_loss: 0.4663 - val_acc: 0.8816\n",
      "Epoch 8/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4758 - acc: 0.8799Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 584us/step - loss: 0.4560 - acc: 0.8845 - val_loss: 0.4818 - val_acc: 0.8816\n",
      "Epoch 9/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4486 - acc: 0.8875Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 1s 732us/step - loss: 0.4537 - acc: 0.8845 - val_loss: 0.4711 - val_acc: 0.8816\n",
      "Epoch 10/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4618 - acc: 0.886 - ETA: 0s - loss: 0.4537 - acc: 0.8869Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 1s 938us/step - loss: 0.4582 - acc: 0.8845 - val_loss: 0.4725 - val_acc: 0.8816\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4674 - acc: 0.8832Epoch 00001: val_loss improved from inf to 0.47919, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 964us/step - loss: 0.4524 - acc: 0.8874 - val_loss: 0.4792 - val_acc: 0.8728\n",
      "Epoch 2/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4491 - acc: 0.8884Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 0s 639us/step - loss: 0.4503 - acc: 0.8874 - val_loss: 0.4795 - val_acc: 0.8728\n",
      "Epoch 3/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4344 - acc: 0.8906Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 0s 545us/step - loss: 0.4473 - acc: 0.8874 - val_loss: 0.4861 - val_acc: 0.8728\n",
      "Epoch 4/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4635 - acc: 0.8828Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 1s 770us/step - loss: 0.4492 - acc: 0.8874 - val_loss: 0.4842 - val_acc: 0.8728\n",
      "Epoch 5/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4478 - acc: 0.8884Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 557us/step - loss: 0.4490 - acc: 0.8874 - val_loss: 0.4796 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4514 - acc: 0.8869Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 718us/step - loss: 0.4490 - acc: 0.8874 - val_loss: 0.4829 - val_acc: 0.8728\n",
      "Epoch 7/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4451 - acc: 0.8884Epoch 00007: val_loss improved from 0.47919 to 0.47850, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 810us/step - loss: 0.4488 - acc: 0.8874 - val_loss: 0.4785 - val_acc: 0.8728\n",
      "Epoch 8/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4433 - acc: 0.8889Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 702us/step - loss: 0.4457 - acc: 0.8874 - val_loss: 0.4810 - val_acc: 0.8728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4499 - acc: 0.8869Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 1s 850us/step - loss: 0.4476 - acc: 0.8874 - val_loss: 0.4844 - val_acc: 0.8728\n",
      "Epoch 10/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4351 - acc: 0.8882Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 1s 838us/step - loss: 0.4479 - acc: 0.8874 - val_loss: 0.4851 - val_acc: 0.8728\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4821 - acc: 0.8734Epoch 00001: val_loss improved from inf to 0.38978, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 718us/step - loss: 0.4774 - acc: 0.8757 - val_loss: 0.3898 - val_acc: 0.9079\n",
      "Epoch 2/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4795 - acc: 0.8750Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 0s 727us/step - loss: 0.4789 - acc: 0.8757 - val_loss: 0.3935 - val_acc: 0.9079\n",
      "Epoch 3/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4766 - acc: 0.8750Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 1s 850us/step - loss: 0.4763 - acc: 0.8757 - val_loss: 0.4017 - val_acc: 0.9079\n",
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4719 - acc: 0.8780Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 1s 860us/step - loss: 0.4782 - acc: 0.8757 - val_loss: 0.4085 - val_acc: 0.9079\n",
      "Epoch 5/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4907 - acc: 0.8688Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 700us/step - loss: 0.4729 - acc: 0.8757 - val_loss: 0.4019 - val_acc: 0.9079\n",
      "Epoch 6/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4627 - acc: 0.8812Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 1s 741us/step - loss: 0.4791 - acc: 0.8757 - val_loss: 0.4112 - val_acc: 0.9079\n",
      "Epoch 7/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4805 - acc: 0.8750Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 674us/step - loss: 0.4777 - acc: 0.8757 - val_loss: 0.4123 - val_acc: 0.9079\n",
      "Epoch 8/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4732 - acc: 0.8765Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 669us/step - loss: 0.4736 - acc: 0.8757 - val_loss: 0.4318 - val_acc: 0.9079\n",
      "Epoch 9/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4874 - acc: 0.8734Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 1s 767us/step - loss: 0.4783 - acc: 0.8757 - val_loss: 0.4221 - val_acc: 0.9079\n",
      "Epoch 10/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4693 - acc: 0.8766Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 687us/step - loss: 0.4766 - acc: 0.8757 - val_loss: 0.4198 - val_acc: 0.9079\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4619 - acc: 0.8849Epoch 00001: val_loss improved from inf to 0.48427, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 849us/step - loss: 0.4552 - acc: 0.8874 - val_loss: 0.4843 - val_acc: 0.8728\n",
      "Epoch 2/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4598 - acc: 0.8869Epoch 00002: val_loss improved from 0.48427 to 0.48081, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 743us/step - loss: 0.4573 - acc: 0.8874 - val_loss: 0.4808 - val_acc: 0.8728\n",
      "Epoch 3/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4645 - acc: 0.8865Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 0s 553us/step - loss: 0.4568 - acc: 0.8874 - val_loss: 0.4819 - val_acc: 0.8728\n",
      "Epoch 4/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4600 - acc: 0.8865Epoch 00004: val_loss improved from 0.48081 to 0.47981, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 629us/step - loss: 0.4576 - acc: 0.8874 - val_loss: 0.4798 - val_acc: 0.8728\n",
      "Epoch 5/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4126 - acc: 0.8976Epoch 00005: val_loss improved from 0.47981 to 0.47945, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 735us/step - loss: 0.4552 - acc: 0.8874 - val_loss: 0.4794 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4576 - acc: 0.8869Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 1s 794us/step - loss: 0.4551 - acc: 0.8874 - val_loss: 0.4847 - val_acc: 0.8728\n",
      "Epoch 7/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4547 - acc: 0.8869Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 725us/step - loss: 0.4544 - acc: 0.8874 - val_loss: 0.4934 - val_acc: 0.8728\n",
      "Epoch 8/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4628 - acc: 0.8854Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 587us/step - loss: 0.4571 - acc: 0.8874 - val_loss: 0.4824 - val_acc: 0.8728\n",
      "Epoch 9/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4618 - acc: 0.8854Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 1s 892us/step - loss: 0.4556 - acc: 0.8874 - val_loss: 0.4880 - val_acc: 0.8728\n",
      "Epoch 10/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4597 - acc: 0.8865Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 568us/step - loss: 0.4554 - acc: 0.8874 - val_loss: 0.4846 - val_acc: 0.8728\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 606us/step - loss: 0.4616 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 655us/step - loss: 0.4616 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 0s 463us/step - loss: 0.4601 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 507us/step - loss: 0.4605 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 520us/step - loss: 0.4579 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 569us/step - loss: 0.4611 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 0s 450us/step - loss: 0.4616 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 559us/step - loss: 0.4608 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 575us/step - loss: 0.4585 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 617us/step - loss: 0.4598 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 613us/step - loss: 0.4585 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 0s 457us/step - loss: 0.4597 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 0s 457us/step - loss: 0.4576 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 533us/step - loss: 0.4587 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 604us/step - loss: 0.4597 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 570us/step - loss: 0.4566 - acc: 0.8838\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912/912 [==============================] - 0s 532us/step - loss: 0.4586 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 645us/step - loss: 0.4574 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 555us/step - loss: 0.4589 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 679us/step - loss: 0.4576 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 676us/step - loss: 0.4578 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 600us/step - loss: 0.4570 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 599us/step - loss: 0.4550 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 608us/step - loss: 0.4584 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 715us/step - loss: 0.4565 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 0s 530us/step - loss: 0.4564 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 634us/step - loss: 0.4569 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 639us/step - loss: 0.4574 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 663us/step - loss: 0.4581 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 572us/step - loss: 0.4558 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 632us/step - loss: 0.4552 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 699us/step - loss: 0.4588 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 590us/step - loss: 0.4574 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 622us/step - loss: 0.4564 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 508us/step - loss: 0.4568 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 0s 544us/step - loss: 0.4576 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 578us/step - loss: 0.4580 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 0s 511us/step - loss: 0.4565 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 654us/step - loss: 0.4580 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 577us/step - loss: 0.4574 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 0s 478us/step - loss: 0.4576 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 568us/step - loss: 0.4575 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 0s 546us/step - loss: 0.4576 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 590us/step - loss: 0.4557 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 611us/step - loss: 0.4576 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 711us/step - loss: 0.4580 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 560us/step - loss: 0.4569 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 667us/step - loss: 0.4570 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 662us/step - loss: 0.4559 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 486us/step - loss: 0.4583 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 594us/step - loss: 0.4570 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 632us/step - loss: 0.4579 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 691us/step - loss: 0.4580 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 621us/step - loss: 0.4570 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 540us/step - loss: 0.4586 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 0s 457us/step - loss: 0.4572 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 0s 459us/step - loss: 0.4585 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 0s 448us/step - loss: 0.4562 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 585us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 463us/step - loss: 0.4572 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "AlgoCrossValIter - 9\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.5206 - acc: 0.8408Epoch 00001: val_loss improved from inf to 0.47834, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 975us/step - loss: 0.5175 - acc: 0.8421 - val_loss: 0.4783 - val_acc: 0.8816\n",
      "Epoch 2/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4853 - acc: 0.8824Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 0s 681us/step - loss: 0.4782 - acc: 0.8845 - val_loss: 0.5079 - val_acc: 0.8816\n",
      "Epoch 3/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4664 - acc: 0.8849Epoch 00003: val_loss improved from 0.47834 to 0.46745, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 800us/step - loss: 0.4732 - acc: 0.8845 - val_loss: 0.4674 - val_acc: 0.8816\n",
      "Epoch 4/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4633 - acc: 0.8854Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 0s 680us/step - loss: 0.4627 - acc: 0.8845 - val_loss: 0.4725 - val_acc: 0.8816\n",
      "Epoch 5/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4691 - acc: 0.8839Epoch 00005: val_loss improved from 0.46745 to 0.46504, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 859us/step - loss: 0.4666 - acc: 0.8845 - val_loss: 0.4650 - val_acc: 0.8816\n",
      "Epoch 6/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4584 - acc: 0.8854Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 525us/step - loss: 0.4619 - acc: 0.8845 - val_loss: 0.4653 - val_acc: 0.8816\n",
      "Epoch 7/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4619 - acc: 0.8839Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 529us/step - loss: 0.4593 - acc: 0.8845 - val_loss: 0.4707 - val_acc: 0.8816\n",
      "Epoch 8/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4577 - acc: 0.8839Epoch 00008: val_loss improved from 0.46504 to 0.46227, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 740us/step - loss: 0.4580 - acc: 0.8845 - val_loss: 0.4623 - val_acc: 0.8816\n",
      "Epoch 9/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4675 - acc: 0.8812Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 702us/step - loss: 0.4558 - acc: 0.8845 - val_loss: 0.4656 - val_acc: 0.8816\n",
      "Epoch 10/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4570 - acc: 0.8859Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 1s 876us/step - loss: 0.4563 - acc: 0.8845 - val_loss: 0.4648 - val_acc: 0.8816\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4646 - acc: 0.8837Epoch 00001: val_loss improved from inf to 0.48375, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 904us/step - loss: 0.4481 - acc: 0.8874 - val_loss: 0.4838 - val_acc: 0.8728\n",
      "Epoch 2/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4537 - acc: 0.8844Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 0s 694us/step - loss: 0.4474 - acc: 0.8874 - val_loss: 0.4901 - val_acc: 0.8728\n",
      "Epoch 3/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4218 - acc: 0.8976Epoch 00003: val_loss improved from 0.48375 to 0.48290, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 821us/step - loss: 0.4474 - acc: 0.8874 - val_loss: 0.4829 - val_acc: 0.8728\n",
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4546 - acc: 0.8854Epoch 00004: val_loss improved from 0.48290 to 0.48266, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 910us/step - loss: 0.4489 - acc: 0.8874 - val_loss: 0.4827 - val_acc: 0.8728\n",
      "Epoch 5/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4258 - acc: 0.8938Epoch 00005: val_loss improved from 0.48266 to 0.48019, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 850us/step - loss: 0.4481 - acc: 0.8874 - val_loss: 0.4802 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4308 - acc: 0.8906Epoch 00006: val_loss improved from 0.48019 to 0.47937, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.4490 - acc: 0.8874 - val_loss: 0.4794 - val_acc: 0.8728\n",
      "Epoch 7/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4545 - acc: 0.8849Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 680us/step - loss: 0.4471 - acc: 0.8874 - val_loss: 0.4869 - val_acc: 0.8728\n",
      "Epoch 8/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4414 - acc: 0.8899Epoch 00008: val_loss improved from 0.47937 to 0.47829, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 765us/step - loss: 0.4487 - acc: 0.8874 - val_loss: 0.4783 - val_acc: 0.8728\n",
      "Epoch 9/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4424 - acc: 0.8875Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 1s 853us/step - loss: 0.4471 - acc: 0.8874 - val_loss: 0.4798 - val_acc: 0.8728\n",
      "Epoch 10/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4527 - acc: 0.8854Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 531us/step - loss: 0.4471 - acc: 0.8874 - val_loss: 0.4812 - val_acc: 0.8728\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4720 - acc: 0.8766Epoch 00001: val_loss improved from inf to 0.40224, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 823us/step - loss: 0.4774 - acc: 0.8757 - val_loss: 0.4022 - val_acc: 0.9079\n",
      "Epoch 2/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4806 - acc: 0.8734Epoch 00002: val_loss improved from 0.40224 to 0.39570, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.4756 - acc: 0.8757 - val_loss: 0.3957 - val_acc: 0.9079\n",
      "Epoch 3/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4812 - acc: 0.8735Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 0s 523us/step - loss: 0.4754 - acc: 0.8757 - val_loss: 0.3992 - val_acc: 0.9079\n",
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4757 - acc: 0.8750Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 1s 758us/step - loss: 0.4759 - acc: 0.8757 - val_loss: 0.4005 - val_acc: 0.9079\n",
      "Epoch 5/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4669 - acc: 0.8767Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 616us/step - loss: 0.4751 - acc: 0.8757 - val_loss: 0.4033 - val_acc: 0.9079\n",
      "Epoch 6/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4832 - acc: 0.8735Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 665us/step - loss: 0.4770 - acc: 0.8757 - val_loss: 0.4064 - val_acc: 0.9079\n",
      "Epoch 7/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4824 - acc: 0.8735Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 517us/step - loss: 0.4762 - acc: 0.8757 - val_loss: 0.4083 - val_acc: 0.9079\n",
      "Epoch 8/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4801 - acc: 0.8750Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 591us/step - loss: 0.4774 - acc: 0.8757 - val_loss: 0.4222 - val_acc: 0.9079\n",
      "Epoch 9/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4791 - acc: 0.8734Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 1s 914us/step - loss: 0.4755 - acc: 0.8757 - val_loss: 0.4193 - val_acc: 0.9079\n",
      "Epoch 10/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4760 - acc: 0.8750Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 663us/step - loss: 0.4755 - acc: 0.8757 - val_loss: 0.4191 - val_acc: 0.9079\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4578 - acc: 0.8869Epoch 00001: val_loss improved from inf to 0.48086, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 656us/step - loss: 0.4553 - acc: 0.8874 - val_loss: 0.4809 - val_acc: 0.8728\n",
      "Epoch 2/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4621 - acc: 0.8849Epoch 00002: val_loss did not improve\n",
      "684/684 [==============================] - 1s 802us/step - loss: 0.4568 - acc: 0.8874 - val_loss: 0.4813 - val_acc: 0.8728\n",
      "Epoch 3/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4532 - acc: 0.8869Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 1s 767us/step - loss: 0.4535 - acc: 0.8874 - val_loss: 0.4898 - val_acc: 0.8728\n",
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4546 - acc: 0.8884Epoch 00004: val_loss improved from 0.48086 to 0.47981, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 587us/step - loss: 0.4557 - acc: 0.8874 - val_loss: 0.4798 - val_acc: 0.8728\n",
      "Epoch 5/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4513 - acc: 0.8872Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 655us/step - loss: 0.4543 - acc: 0.8874 - val_loss: 0.4814 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4395 - acc: 0.8914Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 1s 795us/step - loss: 0.4537 - acc: 0.8874 - val_loss: 0.4835 - val_acc: 0.8728\n",
      "Epoch 7/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4346 - acc: 0.8891Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 714us/step - loss: 0.4548 - acc: 0.8874 - val_loss: 0.4799 - val_acc: 0.8728\n",
      "Epoch 8/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4543 - acc: 0.8869- ETA: 0s - loss: 0.4528 - acc: 0.88Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 1s 931us/step - loss: 0.4542 - acc: 0.8874 - val_loss: 0.4819 - val_acc: 0.8728\n",
      "Epoch 9/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4623 - acc: 0.8854Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 656us/step - loss: 0.4534 - acc: 0.8874 - val_loss: 0.4833 - val_acc: 0.8728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4588 - acc: 0.8854Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 670us/step - loss: 0.4526 - acc: 0.8874 - val_loss: 0.4885 - val_acc: 0.8728\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 775us/step - loss: 0.4611 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 688us/step - loss: 0.4591 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 740us/step - loss: 0.4586 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 728us/step - loss: 0.4595 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 541us/step - loss: 0.4600 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 617us/step - loss: 0.4590 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 551us/step - loss: 0.4586 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 0s 464us/step - loss: 0.4584 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 552us/step - loss: 0.4589 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 570us/step - loss: 0.4578 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 595us/step - loss: 0.4591 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 0s 540us/step - loss: 0.4578 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 736us/step - loss: 0.4572 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 758us/step - loss: 0.4577 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 567us/step - loss: 0.4568 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 0s 488us/step - loss: 0.4566 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 558us/step - loss: 0.4576 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 0s 498us/step - loss: 0.4578 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 0s 527us/step - loss: 0.4570 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 467us/step - loss: 0.4568 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 572us/step - loss: 0.4567 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 0s 525us/step - loss: 0.4559 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 780us/step - loss: 0.4564 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 583us/step - loss: 0.4571 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 613us/step - loss: 0.4565 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 681us/step - loss: 0.4568 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 652us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 822us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 756us/step - loss: 0.4554 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 607us/step - loss: 0.4577 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 606us/step - loss: 0.4564 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 590us/step - loss: 0.4574 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 0s 500us/step - loss: 0.4547 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 697us/step - loss: 0.4570 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 644us/step - loss: 0.4562 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 0s 491us/step - loss: 0.4565 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 617us/step - loss: 0.4565 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 698us/step - loss: 0.4571 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 635us/step - loss: 0.4562 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 469us/step - loss: 0.4563 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 670us/step - loss: 0.4571 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 619us/step - loss: 0.4559 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 0s 539us/step - loss: 0.4542 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 520us/step - loss: 0.4559 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 465us/step - loss: 0.4570 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 0s 466us/step - loss: 0.4561 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 0s 508us/step - loss: 0.4570 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 0s 464us/step - loss: 0.4560 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 0s 507us/step - loss: 0.4559 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 472us/step - loss: 0.4568 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 0s 532us/step - loss: 0.4564 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 0s 462us/step - loss: 0.4562 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 0s 462us/step - loss: 0.4567 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 612us/step - loss: 0.4563 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 731us/step - loss: 0.4565 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 749us/step - loss: 0.4549 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 706us/step - loss: 0.4560 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 0s 538us/step - loss: 0.4548 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 612us/step - loss: 0.4556 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 466us/step - loss: 0.4576 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "AlgoCrossValIter - 10\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.5294 - acc: 0.8816Epoch 00001: val_loss improved from inf to 0.47448, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 918us/step - loss: 0.5211 - acc: 0.8845 - val_loss: 0.4745 - val_acc: 0.8816\n",
      "Epoch 2/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4734 - acc: 0.8854Epoch 00002: val_loss improved from 0.47448 to 0.46950, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 759us/step - loss: 0.4772 - acc: 0.8845 - val_loss: 0.4695 - val_acc: 0.8816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4759 - acc: 0.8849Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 0s 577us/step - loss: 0.4739 - acc: 0.8845 - val_loss: 0.6227 - val_acc: 0.8816\n",
      "Epoch 4/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4951 - acc: 0.8812Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 0s 656us/step - loss: 0.4815 - acc: 0.8845 - val_loss: 0.4860 - val_acc: 0.8816\n",
      "Epoch 5/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4666 - acc: 0.8844Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 1s 801us/step - loss: 0.4655 - acc: 0.8845 - val_loss: 0.4777 - val_acc: 0.8816\n",
      "Epoch 6/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4770 - acc: 0.8816Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 609us/step - loss: 0.4608 - acc: 0.8845 - val_loss: 0.4947 - val_acc: 0.8816\n",
      "Epoch 7/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4738 - acc: 0.8824Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 689us/step - loss: 0.4672 - acc: 0.8845 - val_loss: 0.4807 - val_acc: 0.8816\n",
      "Epoch 8/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4615 - acc: 0.8839Epoch 00008: val_loss improved from 0.46950 to 0.46518, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 875us/step - loss: 0.4590 - acc: 0.8845 - val_loss: 0.4652 - val_acc: 0.8816\n",
      "Epoch 9/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4516 - acc: 0.8854Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 675us/step - loss: 0.4548 - acc: 0.8845 - val_loss: 0.4749 - val_acc: 0.8816\n",
      "Epoch 10/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4578 - acc: 0.8854Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 0s 644us/step - loss: 0.4587 - acc: 0.8845 - val_loss: 0.4686 - val_acc: 0.8816\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4648 - acc: 0.8837Epoch 00001: val_loss improved from inf to 0.48311, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 628us/step - loss: 0.4512 - acc: 0.8874 - val_loss: 0.4831 - val_acc: 0.8728\n",
      "Epoch 2/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4435 - acc: 0.8906Epoch 00002: val_loss improved from 0.48311 to 0.48125, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.4499 - acc: 0.8874 - val_loss: 0.4812 - val_acc: 0.8728\n",
      "Epoch 3/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4522 - acc: 0.8844Epoch 00003: val_loss did not improve\n",
      "684/684 [==============================] - 0s 712us/step - loss: 0.4492 - acc: 0.8874 - val_loss: 0.4939 - val_acc: 0.8728\n",
      "Epoch 4/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4296 - acc: 0.8980Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 1s 794us/step - loss: 0.4495 - acc: 0.8874 - val_loss: 0.4910 - val_acc: 0.8728\n",
      "Epoch 5/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4526 - acc: 0.8875Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 1s 787us/step - loss: 0.4514 - acc: 0.8874 - val_loss: 0.4825 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4732 - acc: 0.8799Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 565us/step - loss: 0.4473 - acc: 0.8874 - val_loss: 0.4953 - val_acc: 0.8728\n",
      "Epoch 7/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4355 - acc: 0.8924Epoch 00007: val_loss improved from 0.48125 to 0.47980, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 727us/step - loss: 0.4488 - acc: 0.8874 - val_loss: 0.4798 - val_acc: 0.8728\n",
      "Epoch 8/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4439 - acc: 0.8865Epoch 00008: val_loss improved from 0.47980 to 0.47900, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 753us/step - loss: 0.4509 - acc: 0.8874 - val_loss: 0.4790 - val_acc: 0.8728\n",
      "Epoch 9/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4470 - acc: 0.8884Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 544us/step - loss: 0.4481 - acc: 0.8874 - val_loss: 0.4799 - val_acc: 0.8728\n",
      "Epoch 10/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4549 - acc: 0.8854Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 1s 918us/step - loss: 0.4491 - acc: 0.8874 - val_loss: 0.4850 - val_acc: 0.8728\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4555 - acc: 0.8837Epoch 00001: val_loss improved from inf to 0.40609, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 641us/step - loss: 0.4774 - acc: 0.8757 - val_loss: 0.4061 - val_acc: 0.9079\n",
      "Epoch 2/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4756 - acc: 0.8766Epoch 00002: val_loss improved from 0.40609 to 0.39630, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.4752 - acc: 0.8757 - val_loss: 0.3963 - val_acc: 0.9079\n",
      "Epoch 3/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4873 - acc: 0.8698Epoch 00003: val_loss improved from 0.39630 to 0.39593, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 815us/step - loss: 0.4752 - acc: 0.8757 - val_loss: 0.3959 - val_acc: 0.9079\n",
      "Epoch 4/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4852 - acc: 0.8717Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 0s 673us/step - loss: 0.4774 - acc: 0.8757 - val_loss: 0.4004 - val_acc: 0.9079\n",
      "Epoch 5/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4792 - acc: 0.8750Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 550us/step - loss: 0.4785 - acc: 0.8757 - val_loss: 0.4078 - val_acc: 0.9079\n",
      "Epoch 6/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4716 - acc: 0.8765Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 0s 524us/step - loss: 0.4767 - acc: 0.8757 - val_loss: 0.4117 - val_acc: 0.9079\n",
      "Epoch 7/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4759 - acc: 0.8766Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 1s 746us/step - loss: 0.4757 - acc: 0.8757 - val_loss: 0.4196 - val_acc: 0.9079\n",
      "Epoch 8/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4690 - acc: 0.8766Epoch 00008: val_loss did not improve\n",
      "684/684 [==============================] - 0s 730us/step - loss: 0.4747 - acc: 0.8757 - val_loss: 0.4177 - val_acc: 0.9079\n",
      "Epoch 9/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4847 - acc: 0.8719Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 548us/step - loss: 0.4753 - acc: 0.8757 - val_loss: 0.4185 - val_acc: 0.9079\n",
      "Epoch 10/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4702 - acc: 0.8781Epoch 00010: val_loss did not improve\n",
      "684/684 [==============================] - 1s 759us/step - loss: 0.4766 - acc: 0.8757 - val_loss: 0.4227 - val_acc: 0.9079\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 684 samples, validate on 228 samples\n",
      "Epoch 1/10\n",
      "608/684 [=========================>....] - ETA: 0s - loss: 0.4766 - acc: 0.8816Epoch 00001: val_loss improved from inf to 0.48966, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 758us/step - loss: 0.4563 - acc: 0.8874 - val_loss: 0.4897 - val_acc: 0.8728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4672 - acc: 0.8844Epoch 00002: val_loss improved from 0.48966 to 0.48119, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 972us/step - loss: 0.4584 - acc: 0.8874 - val_loss: 0.4812 - val_acc: 0.8728\n",
      "Epoch 3/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4556 - acc: 0.8884Epoch 00003: val_loss improved from 0.48119 to 0.48097, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 847us/step - loss: 0.4568 - acc: 0.8874 - val_loss: 0.4810 - val_acc: 0.8728\n",
      "Epoch 4/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4574 - acc: 0.8884- ETA: 0s - loss: 0.4183 - acc: 0.8Epoch 00004: val_loss did not improve\n",
      "684/684 [==============================] - 0s 651us/step - loss: 0.4582 - acc: 0.8874 - val_loss: 0.4814 - val_acc: 0.8728\n",
      "Epoch 5/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4469 - acc: 0.8844Epoch 00005: val_loss did not improve\n",
      "684/684 [==============================] - 0s 659us/step - loss: 0.4555 - acc: 0.8874 - val_loss: 0.4934 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "640/684 [===========================>..] - ETA: 0s - loss: 0.4519 - acc: 0.8891Epoch 00006: val_loss did not improve\n",
      "684/684 [==============================] - 1s 749us/step - loss: 0.4568 - acc: 0.8874 - val_loss: 0.4823 - val_acc: 0.8728\n",
      "Epoch 7/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4813 - acc: 0.8802Epoch 00007: val_loss did not improve\n",
      "684/684 [==============================] - 0s 720us/step - loss: 0.4564 - acc: 0.8874 - val_loss: 0.4839 - val_acc: 0.8728\n",
      "Epoch 8/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4581 - acc: 0.8869Epoch 00008: val_loss improved from 0.48097 to 0.48080, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 0s 699us/step - loss: 0.4557 - acc: 0.8874 - val_loss: 0.4808 - val_acc: 0.8728\n",
      "Epoch 9/10\n",
      "672/684 [============================>.] - ETA: 0s - loss: 0.4563 - acc: 0.8869Epoch 00009: val_loss did not improve\n",
      "684/684 [==============================] - 0s 697us/step - loss: 0.4538 - acc: 0.8874 - val_loss: 0.4884 - val_acc: 0.8728\n",
      "Epoch 10/10\n",
      "576/684 [========================>.....] - ETA: 0s - loss: 0.4539 - acc: 0.8889Epoch 00010: val_loss improved from 0.48080 to 0.47994, saving model to best-model-conll.hdfs\n",
      "684/684 [==============================] - 1s 801us/step - loss: 0.4554 - acc: 0.8874 - val_loss: 0.4799 - val_acc: 0.8728\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 674us/step - loss: 0.4610 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 622us/step - loss: 0.4620 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 665us/step - loss: 0.4617 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 492us/step - loss: 0.4612 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 558us/step - loss: 0.4598 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 786us/step - loss: 0.4589 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 773us/step - loss: 0.4604 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 641us/step - loss: 0.4592 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 559us/step - loss: 0.4594 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 709us/step - loss: 0.4586 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 648us/step - loss: 0.4594 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 685us/step - loss: 0.4577 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 584us/step - loss: 0.4595 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 527us/step - loss: 0.4576 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 528us/step - loss: 0.4578 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 0s 498us/step - loss: 0.4584 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 0s 548us/step - loss: 0.4575 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 565us/step - loss: 0.4575 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 569us/step - loss: 0.4581 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 449us/step - loss: 0.4570 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 0s 504us/step - loss: 0.4545 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 580us/step - loss: 0.4582 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 606us/step - loss: 0.4569 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 468us/step - loss: 0.4579 - acc: 0.8838 0s - loss: 0.4771 - acc: 0.\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 487us/step - loss: 0.4555 - acc: 0.8838 0s - loss: 0.5128 - acc: 0\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 0s 526us/step - loss: 0.4569 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 0s 548us/step - loss: 0.4577 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 0s 485us/step - loss: 0.4568 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 705us/step - loss: 0.4561 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 518us/step - loss: 0.4570 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 591us/step - loss: 0.4568 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 550us/step - loss: 0.4575 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 0s 475us/step - loss: 0.4559 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 451us/step - loss: 0.4553 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 518us/step - loss: 0.4572 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 0s 492us/step - loss: 0.4572 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 0s 509us/step - loss: 0.4551 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 0s 489us/step - loss: 0.4576 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 0s 507us/step - loss: 0.4566 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 0s 461us/step - loss: 0.4574 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 575us/step - loss: 0.4566 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 554us/step - loss: 0.4567 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 0s 474us/step - loss: 0.4562 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 0s 446us/step - loss: 0.4564 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 0s 464us/step - loss: 0.4556 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 615us/step - loss: 0.4572 - acc: 0.8838\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912/912 [==============================] - 1s 693us/step - loss: 0.4551 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 610us/step - loss: 0.4561 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 556us/step - loss: 0.4569 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 598us/step - loss: 0.4568 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n",
      "Train on 912 samples, validate on 0 samples\n",
      "Epoch 1/10\n",
      "912/912 [==============================] - 1s 657us/step - loss: 0.4563 - acc: 0.8838\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 1s 692us/step - loss: 0.4574 - acc: 0.8838\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 1s 597us/step - loss: 0.4549 - acc: 0.8838\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 1s 597us/step - loss: 0.4574 - acc: 0.8838\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 1s 643us/step - loss: 0.4582 - acc: 0.8838\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 1s 562us/step - loss: 0.4559 - acc: 0.8838\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 1s 608us/step - loss: 0.4567 - acc: 0.8838\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 1s 679us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 1s 702us/step - loss: 0.4573 - acc: 0.8838\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 1s 605us/step - loss: 0.4571 - acc: 0.8838\n",
      "number of correct positive predictions is 0\n",
      "number of correct positive predictions is 0\n",
      "number of position exple is 0\n",
      "Recall and precision are 0\n",
      "number of correct positive predictions is 0\n"
     ]
    }
   ],
   "source": [
    "m = create_model(X_encoded.shape[1], len(tagSet))\n",
    "resultCrossVal, trainByTagResult, testByTagResult, ewoByTagResult = algoCrossVal(m, X_encoded, y, X_ewo_encoded, y_ewo, repeat=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1-ewo</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-test</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-train</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_ewo</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_test</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_train</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_ewo</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_test</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_train</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0    0    0    0    0    0    0    0    0    0\n",
       "F1-ewo    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
       "F1-test   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "F1-train  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
       "P_ewo     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "P_test    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "P_train   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "R_ewo     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "R_test    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "R_train   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if with_encoding:\n",
    "    resultCrossVal.to_csv(\"results/merge-{}-encoding.csv\".format(max_depth))\n",
    "else:\n",
    "    resultCrossVal.to_csv(\"results/merge-{}-no-encoding.csv\".format(max_depth))\n",
    "resultCrossVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1-ewo</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-test</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-train</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_ewo</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_test</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_train</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_ewo</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_test</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_train</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "F1-ewo    NaN\n",
       "F1-test   0.0\n",
       "F1-train  NaN\n",
       "P_ewo     0.0\n",
       "P_test    0.0\n",
       "P_train   0.0\n",
       "R_ewo     0.0\n",
       "R_test    0.0\n",
       "R_train   0.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultCrossVal.mean(axis=1).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1-ewo</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-test</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-train</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_ewo</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_test</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_train</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_ewo</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_test</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_train</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "F1-ewo    NaN\n",
       "F1-test   0.0\n",
       "F1-train  NaN\n",
       "P_ewo     0.0\n",
       "P_test    0.0\n",
       "P_train   0.0\n",
       "R_ewo     0.0\n",
       "R_test    0.0\n",
       "R_train   0.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultCrossVal.std(axis=1).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1-LOC</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-MISC</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-O</th>\n",
       "      <td>93.828</td>\n",
       "      <td>93.828</td>\n",
       "      <td>93.828</td>\n",
       "      <td>93.828</td>\n",
       "      <td>93.828</td>\n",
       "      <td>93.828</td>\n",
       "      <td>93.828</td>\n",
       "      <td>93.828</td>\n",
       "      <td>93.828</td>\n",
       "      <td>93.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-ORG</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-PER</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-LOC</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-MISC</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-O</th>\n",
       "      <td>88.378</td>\n",
       "      <td>88.378</td>\n",
       "      <td>88.378</td>\n",
       "      <td>88.378</td>\n",
       "      <td>88.378</td>\n",
       "      <td>88.378</td>\n",
       "      <td>88.378</td>\n",
       "      <td>88.378</td>\n",
       "      <td>88.378</td>\n",
       "      <td>88.378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-ORG</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-PER</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-LOC</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-MISC</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-O</th>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-ORG</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-PER</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0        0        0        0        0        0        0  \\\n",
       "F1-LOC       NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "F1-MISC      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "F1-O      93.828   93.828   93.828   93.828   93.828   93.828   93.828   \n",
       "F1-ORG     0.000    0.000    0.000    0.000    0.000    0.000    0.000   \n",
       "F1-PER       NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "P-LOC      0.000    0.000    0.000    0.000    0.000    0.000    0.000   \n",
       "P-MISC     0.000    0.000    0.000    0.000    0.000    0.000    0.000   \n",
       "P-O       88.378   88.378   88.378   88.378   88.378   88.378   88.378   \n",
       "P-ORG      0.000    0.000    0.000    0.000    0.000    0.000    0.000   \n",
       "P-PER      0.000    0.000    0.000    0.000    0.000    0.000    0.000   \n",
       "R-LOC      0.000    0.000    0.000    0.000    0.000    0.000    0.000   \n",
       "R-MISC     0.000    0.000    0.000    0.000    0.000    0.000    0.000   \n",
       "R-O      100.000  100.000  100.000  100.000  100.000  100.000  100.000   \n",
       "R-ORG      0.000    0.000    0.000    0.000    0.000    0.000    0.000   \n",
       "R-PER      0.000    0.000    0.000    0.000    0.000    0.000    0.000   \n",
       "\n",
       "               0        0        0  \n",
       "F1-LOC       NaN      NaN      NaN  \n",
       "F1-MISC      NaN      NaN      NaN  \n",
       "F1-O      93.828   93.828   93.828  \n",
       "F1-ORG     0.000    0.000    0.000  \n",
       "F1-PER       NaN      NaN      NaN  \n",
       "P-LOC      0.000    0.000    0.000  \n",
       "P-MISC     0.000    0.000    0.000  \n",
       "P-O       88.378   88.378   88.378  \n",
       "P-ORG      0.000    0.000    0.000  \n",
       "P-PER      0.000    0.000    0.000  \n",
       "R-LOC      0.000    0.000    0.000  \n",
       "R-MISC     0.000    0.000    0.000  \n",
       "R-O      100.000  100.000  100.000  \n",
       "R-ORG      0.000    0.000    0.000  \n",
       "R-PER      0.000    0.000    0.000  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if with_encoding:\n",
    "    trainByTagResult.to_csv(\"results/train-by-tag-merge-{}-encoding.csv\".format(max_depth))\n",
    "else:\n",
    "    trainByTagResult.to_csv(\"results/train-by-tag-merge-{}-no-encoding.csv\".format(max_depth))\n",
    "trainByTagResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1-LOC</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-MISC</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-O</th>\n",
       "      <td>93.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-ORG</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-PER</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-LOC</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-MISC</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-O</th>\n",
       "      <td>88.378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-ORG</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-PER</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-LOC</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-MISC</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-O</th>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-ORG</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-PER</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "F1-LOC       NaN\n",
       "F1-MISC      NaN\n",
       "F1-O      93.828\n",
       "F1-ORG     0.000\n",
       "F1-PER       NaN\n",
       "P-LOC      0.000\n",
       "P-MISC     0.000\n",
       "P-O       88.378\n",
       "P-ORG      0.000\n",
       "P-PER      0.000\n",
       "R-LOC      0.000\n",
       "R-MISC     0.000\n",
       "R-O      100.000\n",
       "R-ORG      0.000\n",
       "R-PER      0.000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainByTagResult.mean(axis=1).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1-LOC</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-MISC</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-O</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-ORG</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-PER</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-LOC</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-MISC</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-O</th>\n",
       "      <td>1.497956e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-ORG</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-PER</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-LOC</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-MISC</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-O</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-ORG</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-PER</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0\n",
       "F1-LOC            NaN\n",
       "F1-MISC           NaN\n",
       "F1-O     0.000000e+00\n",
       "F1-ORG   0.000000e+00\n",
       "F1-PER            NaN\n",
       "P-LOC    0.000000e+00\n",
       "P-MISC   0.000000e+00\n",
       "P-O      1.497956e-14\n",
       "P-ORG    0.000000e+00\n",
       "P-PER    0.000000e+00\n",
       "R-LOC    0.000000e+00\n",
       "R-MISC   0.000000e+00\n",
       "R-O      0.000000e+00\n",
       "R-ORG    0.000000e+00\n",
       "R-PER    0.000000e+00"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainByTagResult.std(axis=1).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1-LOC</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-MISC</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-O</th>\n",
       "      <td>37.530</td>\n",
       "      <td>37.530</td>\n",
       "      <td>37.530</td>\n",
       "      <td>37.530</td>\n",
       "      <td>37.530</td>\n",
       "      <td>37.530</td>\n",
       "      <td>37.530</td>\n",
       "      <td>37.530</td>\n",
       "      <td>37.530</td>\n",
       "      <td>37.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-ORG</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-PER</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-LOC</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-MISC</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-O</th>\n",
       "      <td>35.351</td>\n",
       "      <td>35.351</td>\n",
       "      <td>35.351</td>\n",
       "      <td>35.351</td>\n",
       "      <td>35.351</td>\n",
       "      <td>35.351</td>\n",
       "      <td>35.351</td>\n",
       "      <td>35.351</td>\n",
       "      <td>35.351</td>\n",
       "      <td>35.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-ORG</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-PER</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-LOC</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-MISC</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-O</th>\n",
       "      <td>40.000</td>\n",
       "      <td>40.000</td>\n",
       "      <td>40.000</td>\n",
       "      <td>40.000</td>\n",
       "      <td>40.000</td>\n",
       "      <td>40.000</td>\n",
       "      <td>40.000</td>\n",
       "      <td>40.000</td>\n",
       "      <td>40.000</td>\n",
       "      <td>40.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-ORG</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-PER</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0       0       0       0       0       0       0       0  \\\n",
       "F1-LOC    0.000   0.000   0.000   0.000   0.000   0.000   0.000   0.000   \n",
       "F1-MISC   0.000   0.000   0.000   0.000   0.000   0.000   0.000   0.000   \n",
       "F1-O     37.530  37.530  37.530  37.530  37.530  37.530  37.530  37.530   \n",
       "F1-ORG    0.000   0.000   0.000   0.000   0.000   0.000   0.000   0.000   \n",
       "F1-PER    0.000   0.000   0.000   0.000   0.000   0.000   0.000   0.000   \n",
       "P-LOC     0.000   0.000   0.000   0.000   0.000   0.000   0.000   0.000   \n",
       "P-MISC    0.000   0.000   0.000   0.000   0.000   0.000   0.000   0.000   \n",
       "P-O      35.351  35.351  35.351  35.351  35.351  35.351  35.351  35.351   \n",
       "P-ORG     0.000   0.000   0.000   0.000   0.000   0.000   0.000   0.000   \n",
       "P-PER     0.000   0.000   0.000   0.000   0.000   0.000   0.000   0.000   \n",
       "R-LOC     0.000   0.000   0.000   0.000   0.000   0.000   0.000   0.000   \n",
       "R-MISC    0.000   0.000   0.000   0.000   0.000   0.000   0.000   0.000   \n",
       "R-O      40.000  40.000  40.000  40.000  40.000  40.000  40.000  40.000   \n",
       "R-ORG     0.000   0.000   0.000   0.000   0.000   0.000   0.000   0.000   \n",
       "R-PER     0.000   0.000   0.000   0.000   0.000   0.000   0.000   0.000   \n",
       "\n",
       "              0       0  \n",
       "F1-LOC    0.000   0.000  \n",
       "F1-MISC   0.000   0.000  \n",
       "F1-O     37.530  37.530  \n",
       "F1-ORG    0.000   0.000  \n",
       "F1-PER    0.000   0.000  \n",
       "P-LOC     0.000   0.000  \n",
       "P-MISC    0.000   0.000  \n",
       "P-O      35.351  35.351  \n",
       "P-ORG     0.000   0.000  \n",
       "P-PER     0.000   0.000  \n",
       "R-LOC     0.000   0.000  \n",
       "R-MISC    0.000   0.000  \n",
       "R-O      40.000  40.000  \n",
       "R-ORG     0.000   0.000  \n",
       "R-PER     0.000   0.000  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if with_encoding:\n",
    "    testByTagResult.to_csv(\"results/test-by-tag-merge-{}-encoding.csv\".format(max_depth))\n",
    "else:\n",
    "    testByTagResult.to_csv(\"results/test-by-tag-merge-{}-no-encoding.csv\".format(max_depth))\n",
    "testByTagResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1-LOC</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-MISC</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-O</th>\n",
       "      <td>37.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-ORG</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-PER</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-LOC</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-MISC</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-O</th>\n",
       "      <td>35.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-ORG</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-PER</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-LOC</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-MISC</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-O</th>\n",
       "      <td>40.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-ORG</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-PER</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "F1-LOC    0.000\n",
       "F1-MISC   0.000\n",
       "F1-O     37.530\n",
       "F1-ORG    0.000\n",
       "F1-PER    0.000\n",
       "P-LOC     0.000\n",
       "P-MISC    0.000\n",
       "P-O      35.351\n",
       "P-ORG     0.000\n",
       "P-PER     0.000\n",
       "R-LOC     0.000\n",
       "R-MISC    0.000\n",
       "R-O      40.000\n",
       "R-ORG     0.000\n",
       "R-PER     0.000"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testByTagResult.mean(axis=1).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1-LOC</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-MISC</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-O</th>\n",
       "      <td>7.489778e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-ORG</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-PER</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-LOC</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-MISC</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-O</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-ORG</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-PER</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-LOC</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-MISC</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-O</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-ORG</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-PER</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0\n",
       "F1-LOC   0.000000e+00\n",
       "F1-MISC  0.000000e+00\n",
       "F1-O     7.489778e-15\n",
       "F1-ORG   0.000000e+00\n",
       "F1-PER   0.000000e+00\n",
       "P-LOC    0.000000e+00\n",
       "P-MISC   0.000000e+00\n",
       "P-O      0.000000e+00\n",
       "P-ORG    0.000000e+00\n",
       "P-PER    0.000000e+00\n",
       "R-LOC    0.000000e+00\n",
       "R-MISC   0.000000e+00\n",
       "R-O      0.000000e+00\n",
       "R-ORG    0.000000e+00\n",
       "R-PER    0.000000e+00"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testByTagResult.std(axis=1).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1-LOC</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-MISC</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-O</th>\n",
       "      <td>94.46</td>\n",
       "      <td>94.46</td>\n",
       "      <td>94.46</td>\n",
       "      <td>94.46</td>\n",
       "      <td>94.46</td>\n",
       "      <td>94.46</td>\n",
       "      <td>94.46</td>\n",
       "      <td>94.46</td>\n",
       "      <td>94.46</td>\n",
       "      <td>94.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-ORG</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-PER</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-LOC</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-MISC</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-O</th>\n",
       "      <td>89.50</td>\n",
       "      <td>89.50</td>\n",
       "      <td>89.50</td>\n",
       "      <td>89.50</td>\n",
       "      <td>89.50</td>\n",
       "      <td>89.50</td>\n",
       "      <td>89.50</td>\n",
       "      <td>89.50</td>\n",
       "      <td>89.50</td>\n",
       "      <td>89.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-ORG</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-PER</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-LOC</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-MISC</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-O</th>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-ORG</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-PER</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0       0       0       0       0       0       0       0  \\\n",
       "F1-LOC      NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "F1-MISC     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "F1-O      94.46   94.46   94.46   94.46   94.46   94.46   94.46   94.46   \n",
       "F1-ORG      NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "F1-PER      NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "P-LOC      0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   \n",
       "P-MISC     0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   \n",
       "P-O       89.50   89.50   89.50   89.50   89.50   89.50   89.50   89.50   \n",
       "P-ORG      0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   \n",
       "P-PER      0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   \n",
       "R-LOC      0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   \n",
       "R-MISC     0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   \n",
       "R-O      100.00  100.00  100.00  100.00  100.00  100.00  100.00  100.00   \n",
       "R-ORG      0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   \n",
       "R-PER      0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   \n",
       "\n",
       "              0       0  \n",
       "F1-LOC      NaN     NaN  \n",
       "F1-MISC     NaN     NaN  \n",
       "F1-O      94.46   94.46  \n",
       "F1-ORG      NaN     NaN  \n",
       "F1-PER      NaN     NaN  \n",
       "P-LOC      0.00    0.00  \n",
       "P-MISC     0.00    0.00  \n",
       "P-O       89.50   89.50  \n",
       "P-ORG      0.00    0.00  \n",
       "P-PER      0.00    0.00  \n",
       "R-LOC      0.00    0.00  \n",
       "R-MISC     0.00    0.00  \n",
       "R-O      100.00  100.00  \n",
       "R-ORG      0.00    0.00  \n",
       "R-PER      0.00    0.00  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if with_encoding:\n",
    "    ewoByTagResult.to_csv(\"results/ewo-by-tag-merge-{}-encoding.csv\".format(max_depth))\n",
    "else:\n",
    "    ewoByTagResult.to_csv(\"results/ewo-by-tag-merge-{}-no-encoding.csv\".format(max_depth))\n",
    "ewoByTagResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1-LOC</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-MISC</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-O</th>\n",
       "      <td>94.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-ORG</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-PER</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-LOC</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-MISC</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-O</th>\n",
       "      <td>89.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-ORG</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-PER</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-LOC</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-MISC</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-O</th>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-ORG</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-PER</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "F1-LOC      NaN\n",
       "F1-MISC     NaN\n",
       "F1-O      94.46\n",
       "F1-ORG      NaN\n",
       "F1-PER      NaN\n",
       "P-LOC      0.00\n",
       "P-MISC     0.00\n",
       "P-O       89.50\n",
       "P-ORG      0.00\n",
       "P-PER      0.00\n",
       "R-LOC      0.00\n",
       "R-MISC     0.00\n",
       "R-O      100.00\n",
       "R-ORG      0.00\n",
       "R-PER      0.00"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ewoByTagResult.mean(axis=1).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1-LOC</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-MISC</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-O</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-ORG</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-PER</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-LOC</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-MISC</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-O</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-ORG</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-PER</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-LOC</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-MISC</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-O</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-ORG</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-PER</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "F1-LOC   NaN\n",
       "F1-MISC  NaN\n",
       "F1-O     0.0\n",
       "F1-ORG   NaN\n",
       "F1-PER   NaN\n",
       "P-LOC    0.0\n",
       "P-MISC   0.0\n",
       "P-O      0.0\n",
       "P-ORG    0.0\n",
       "P-PER    0.0\n",
       "R-LOC    0.0\n",
       "R-MISC   0.0\n",
       "R-O      0.0\n",
       "R-ORG    0.0\n",
       "R-PER    0.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ewoByTagResult.std(axis=1).to_frame()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "rnn-ner.ipynb",
   "provenance": [
    {
     "file_id": "1bSiRRO29rixupIV6ume9T9B4KUKtYVKI",
     "timestamp": 1513688449690
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
